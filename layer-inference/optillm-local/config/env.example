# OptiLLM Local env (Orin)
OPTILLM_API_KEY=change-me
OPTILLM_MODEL=Qwen/Qwen2.5-14B-Instruct

# Caches (recommended)
HF_HOME=/opt/homelab/.cache/huggingface
TRANSFORMERS_CACHE=/opt/homelab/.cache/huggingface
TORCH_HOME=/opt/homelab/.cache/torch

# Optional
CUDA_VISIBLE_DEVICES=0
PYTHONPATH=/opt/homelab/optillm-local/layer-inference/optillm-local/runtime
OPTILLM_DISABLE_SKLEARN=1
TRANSFORMERS_NO_SKLEARN=1
