# Architecture: Orin LLM Server

Planned as a dedicated inference backend. Integration will be routed through
LiteLLM once deployed.
