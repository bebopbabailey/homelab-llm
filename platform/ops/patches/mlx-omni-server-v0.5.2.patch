diff --git a/src/mlx_omni_server/chat/mlx/model_types.py b/src/mlx_omni_server/chat/mlx/model_types.py
index 298d6d2..347eba3 100644
--- a/src/mlx_omni_server/chat/mlx/model_types.py
+++ b/src/mlx_omni_server/chat/mlx/model_types.py
@@ -1,5 +1,7 @@
 """MLX Model types and management."""
 
+import json
+import os
 from pathlib import Path
 from typing import Optional, Union
 
@@ -58,17 +60,46 @@ def load_mlx_model(
 
     model_id = model_id.strip()
 
+    # Normalize LiteLLM-style provider prefix.
+    if model_id.startswith("openai/"):
+        model_id = model_id[len("openai/") :]
+
+    # Resolve through homelab MLX registry when available.
+    reg_path = Path(os.environ.get("MLX_OMNI_REGISTRY_PATH", "/Users/thestudio/models/hf/hub/registry.json"))
+    resolved_model_ref = model_id
+    if reg_path.exists():
+        try:
+            data = json.loads(reg_path.read_text() or "{}")
+            models = data.get("models") or {}
+            # Key lookup
+            entry = models.get(model_id)
+            if entry and entry.get("cache_path"):
+                resolved_model_ref = entry["cache_path"]
+            else:
+                for slug, e in models.items():
+                    if (
+                        e.get("model_id") == model_id
+                        or slug == model_id
+                        or e.get("repo_id") == model_id
+                    ):
+                        if e.get("cache_path"):
+                            resolved_model_ref = e["cache_path"]
+                            break
+        except Exception as exc:
+            logger.warning(f"failed to resolve model via registry: {exc}")
+
     try:
         # Load the main model
         model, tokenizer = load(
-            model_id,
+            resolved_model_ref,
             tokenizer_config={"trust_remote_code": True},
             adapter_path=adapter_path,
         )
         logger.info(f"Loaded model: {model_id}")
 
         # Load configuration and create chat tokenizer
-        model_path = get_model_path(model_id)
+        resolved_path = Path(resolved_model_ref)
+        model_path = resolved_path if resolved_path.exists() else get_model_path(model_id)
         config = load_config(model_path)
         chat_template = ChatTemplate(config["model_type"], tokenizer)
 
diff --git a/src/mlx_omni_server/chat/mlx/wrapper_cache.py b/src/mlx_omni_server/chat/mlx/wrapper_cache.py
index db9bea5..0bf5ba1 100644
--- a/src/mlx_omni_server/chat/mlx/wrapper_cache.py
+++ b/src/mlx_omni_server/chat/mlx/wrapper_cache.py
@@ -11,6 +11,8 @@ from collections import OrderedDict
 from dataclasses import dataclass
 from typing import Dict, Optional
 
+import os
+
 from ...utils.logger import logger
 from .chat_generator import ChatGenerator
 
@@ -312,4 +314,19 @@ class MLXWrapperCache:
 
 # Global cache instance - shared across all API endpoints
 # Default to 3 models with 5-minute TTL as suggested by user requirements
-wrapper_cache = MLXWrapperCache(max_size=3, ttl_seconds=300)
+def _env_int(name: str, default: int) -> int:
+    raw = os.environ.get(name)
+    if raw is None or raw == "":
+        return default
+    try:
+        return int(raw)
+    except ValueError:
+        return default
+
+
+# Allow ops to tune caching without patching the code on-host.
+# ttl_seconds <= 0 disables TTL eviction.
+wrapper_cache = MLXWrapperCache(
+    max_size=_env_int("MLX_OMNI_CACHE_MAX_SIZE", 3),
+    ttl_seconds=_env_int("MLX_OMNI_CACHE_TTL_SECONDS", 300),
+)
diff --git a/src/mlx_omni_server/chat/openai/models/models_service.py b/src/mlx_omni_server/chat/openai/models/models_service.py
index af151ca..1cf095e 100644
--- a/src/mlx_omni_server/chat/openai/models/models_service.py
+++ b/src/mlx_omni_server/chat/openai/models/models_service.py
@@ -1,6 +1,8 @@
 import importlib
 import json
 import logging
+import os
+from pathlib import Path
 from typing import Dict, List, Optional, Tuple, Type
 
 from huggingface_hub import CachedRepoInfo, scan_cache_dir
@@ -155,6 +157,9 @@ class ModelsService:
     def __init__(self):
         self.scanner = ModelCacheScanner()
         self.available_models = self._scan_models()
+        self._registry_path = Path(
+            os.environ.get("MLX_OMNI_REGISTRY_PATH", "/Users/thestudio/models/hf/hub/registry.json")
+        )
 
     def _scan_models(self) -> List[Tuple[CachedRepoInfo, Dict]]:
         """Scan local cache for available CausalLM models"""
@@ -164,6 +169,26 @@ class ModelsService:
             print(f"Error scanning cache: {str(e)}")
             return []
 
+    def _list_registry_models(self) -> Optional[ModelList]:
+        """Prefer the homelab MLX registry for model IDs when present.
+
+        Converted models often don't look like HuggingFace repos, so HF cache scanning
+        can miss them. The registry is the source of truth for what we intend to serve.
+        """
+        if not self._registry_path.exists():
+            return None
+        try:
+            data = json.loads(self._registry_path.read_text() or "{}")
+            rows = data.get("models") or {}
+            models: List[Model] = []
+            for slug, entry in rows.items():
+                model_id = entry.get("model_id") or slug
+                models.append(Model(id=model_id, created=0, owned_by="mlx"))
+            return ModelList(data=models)
+        except Exception as exc:
+            logger.warning(f"failed to read MLX registry for /v1/models: {exc}")
+            return None
+
     @staticmethod
     def _get_model_owner(model_id: str) -> str:
         """Extract owner from model ID (part before the /)"""
@@ -171,6 +196,9 @@ class ModelsService:
 
     def list_models(self, include_details: bool = False) -> ModelList:
         """List all available models"""
+        reg = self._list_registry_models()
+        if reg is not None:
+            return reg
         models = []
         for repo_info, config_data in self.available_models:
             model_kwargs = {
diff --git a/src/mlx_omni_server/chat/openai/openai_adapter.py b/src/mlx_omni_server/chat/openai/openai_adapter.py
index f8e099e..668b54f 100644
--- a/src/mlx_omni_server/chat/openai/openai_adapter.py
+++ b/src/mlx_omni_server/chat/openai/openai_adapter.py
@@ -165,8 +165,9 @@ class OpenAIAdapter:
                 for tool in request.tools
             ]
 
-        logger.info(f"messages: {messages}")
-        logger.info(f"template_kwargs: {template_kwargs}")
+        # Avoid logging full prompts/kwargs at info level (can be large and may contain secrets).
+        logger.debug(f"messages_count={len(messages)}")
+        logger.debug(f"template_kwargs_keys={sorted(template_kwargs.keys())}")
 
         json_schema = None
         if request.response_format and request.response_format.json_schema:
diff --git a/src/mlx_omni_server/chat/openai/router.py b/src/mlx_omni_server/chat/openai/router.py
index 5881888..9d8e0c8 100644
--- a/src/mlx_omni_server/chat/openai/router.py
+++ b/src/mlx_omni_server/chat/openai/router.py
@@ -1,3 +1,5 @@
+import os
+import threading
 import json
 from typing import Generator, Optional
 
@@ -14,6 +16,29 @@ from mlx_omni_server.chat.openai.schema import (
 router = APIRouter(tags=["chatâ€”completions"])
 
 
+def _prewarm_models_async() -> None:
+    models = os.environ.get("MLX_OMNI_PREWARM_MODELS", "").strip()
+    if not models:
+        return
+
+    def _run() -> None:
+        for model_id in [m.strip() for m in models.split(",") if m.strip()]:
+            try:
+                ChatGenerator.get_or_create(
+                    model_id=model_id,
+                    adapter_path=None,
+                    draft_model_id=None,
+                )
+            except Exception:
+                # Prewarm failures should not prevent the server from serving.
+                pass
+
+    threading.Thread(target=_run, daemon=True).start()
+
+
+_prewarm_models_async()
+
+
 @router.post("/chat/completions", response_model=ChatCompletionResponse)
 @router.post("/v1/chat/completions", response_model=ChatCompletionResponse)
 async def create_chat_completion(request: ChatCompletionRequest):
