diff --git a/src/mlx_omni_server/chat/mlx/model_types.py b/src/mlx_omni_server/chat/mlx/model_types.py
index 298d6d2..347eba3 100644
--- a/src/mlx_omni_server/chat/mlx/model_types.py
+++ b/src/mlx_omni_server/chat/mlx/model_types.py
@@ -1,5 +1,7 @@
 """MLX Model types and management."""
 
+import json
+import os
 from pathlib import Path
 from typing import Optional, Union
 
@@ -58,17 +60,46 @@ def load_mlx_model(
 
     model_id = model_id.strip()
 
+    # Normalize LiteLLM-style provider prefix.
+    if model_id.startswith("openai/"):
+        model_id = model_id[len("openai/") :]
+
+    # Resolve through homelab MLX registry when available.
+    reg_path = Path(os.environ.get("MLX_OMNI_REGISTRY_PATH", "/Users/thestudio/models/hf/hub/registry.json"))
+    resolved_model_ref = model_id
+    if reg_path.exists():
+        try:
+            data = json.loads(reg_path.read_text() or "{}")
+            models = data.get("models") or {}
+            # Key lookup
+            entry = models.get(model_id)
+            if entry and entry.get("cache_path"):
+                resolved_model_ref = entry["cache_path"]
+            else:
+                for slug, e in models.items():
+                    if (
+                        e.get("model_id") == model_id
+                        or slug == model_id
+                        or e.get("repo_id") == model_id
+                    ):
+                        if e.get("cache_path"):
+                            resolved_model_ref = e["cache_path"]
+                            break
+        except Exception as exc:
+            logger.warning(f"failed to resolve model via registry: {exc}")
+
     try:
         # Load the main model
         model, tokenizer = load(
-            model_id,
+            resolved_model_ref,
             tokenizer_config={"trust_remote_code": True},
             adapter_path=adapter_path,
         )
         logger.info(f"Loaded model: {model_id}")
 
         # Load configuration and create chat tokenizer
-        model_path = get_model_path(model_id)
+        resolved_path = Path(resolved_model_ref)
+        model_path = resolved_path if resolved_path.exists() else get_model_path(model_id)
         config = load_config(model_path)
         chat_template = ChatTemplate(config["model_type"], tokenizer)
 
diff --git a/src/mlx_omni_server/chat/mlx/wrapper_cache.py b/src/mlx_omni_server/chat/mlx/wrapper_cache.py
index db9bea5..0bf5ba1 100644
--- a/src/mlx_omni_server/chat/mlx/wrapper_cache.py
+++ b/src/mlx_omni_server/chat/mlx/wrapper_cache.py
@@ -11,6 +11,8 @@ from collections import OrderedDict
 from dataclasses import dataclass
 from typing import Dict, Optional
 
+import os
+
 from ...utils.logger import logger
 from .chat_generator import ChatGenerator
 
@@ -312,4 +314,19 @@ class MLXWrapperCache:
 
 # Global cache instance - shared across all API endpoints
 # Default to 3 models with 5-minute TTL as suggested by user requirements
-wrapper_cache = MLXWrapperCache(max_size=3, ttl_seconds=300)
+def _env_int(name: str, default: int) -> int:
+    raw = os.environ.get(name)
+    if raw is None or raw == "":
+        return default
+    try:
+        return int(raw)
+    except ValueError:
+        return default
+
+
+# Allow ops to tune caching without patching the code on-host.
+# ttl_seconds <= 0 disables TTL eviction.
+wrapper_cache = MLXWrapperCache(
+    max_size=_env_int("MLX_OMNI_CACHE_MAX_SIZE", 3),
+    ttl_seconds=_env_int("MLX_OMNI_CACHE_TTL_SECONDS", 300),
+)
diff --git a/src/mlx_omni_server/chat/openai/models/models_service.py b/src/mlx_omni_server/chat/openai/models/models_service.py
index af151ca..1cf095e 100644
--- a/src/mlx_omni_server/chat/openai/models/models_service.py
+++ b/src/mlx_omni_server/chat/openai/models/models_service.py
@@ -1,6 +1,8 @@
 import importlib
 import json
 import logging
+import os
+from pathlib import Path
 from typing import Dict, List, Optional, Tuple, Type
 
 from huggingface_hub import CachedRepoInfo, scan_cache_dir
@@ -155,6 +157,9 @@ class ModelsService:
     def __init__(self):
         self.scanner = ModelCacheScanner()
         self.available_models = self._scan_models()
+        self._registry_path = Path(
+            os.environ.get("MLX_OMNI_REGISTRY_PATH", "/Users/thestudio/models/hf/hub/registry.json")
+        )
 
     def _scan_models(self) -> List[Tuple[CachedRepoInfo, Dict]]:
         """Scan local cache for available CausalLM models"""
@@ -164,6 +169,26 @@ class ModelsService:
             print(f"Error scanning cache: {str(e)}")
             return []
 
+    def _list_registry_models(self) -> Optional[ModelList]:
+        """Prefer the homelab MLX registry for model IDs when present.
+
+        Converted models often don't look like HuggingFace repos, so HF cache scanning
+        can miss them. The registry is the source of truth for what we intend to serve.
+        """
+        if not self._registry_path.exists():
+            return None
+        try:
+            data = json.loads(self._registry_path.read_text() or "{}")
+            rows = data.get("models") or {}
+            models: List[Model] = []
+            for slug, entry in rows.items():
+                model_id = entry.get("model_id") or slug
+                models.append(Model(id=model_id, created=0, owned_by="mlx"))
+            return ModelList(data=models)
+        except Exception as exc:
+            logger.warning(f"failed to read MLX registry for /v1/models: {exc}")
+            return None
+
     @staticmethod
     def _get_model_owner(model_id: str) -> str:
         """Extract owner from model ID (part before the /)"""
@@ -171,6 +196,9 @@ class ModelsService:
 
     def list_models(self, include_details: bool = False) -> ModelList:
         """List all available models"""
+        reg = self._list_registry_models()
+        if reg is not None:
+            return reg
         models = []
         for repo_info, config_data in self.available_models:
             model_kwargs = {
diff --git a/src/mlx_omni_server/chat/openai/openai_adapter.py b/src/mlx_omni_server/chat/openai/openai_adapter.py
index f8e099e..668b54f 100644
--- a/src/mlx_omni_server/chat/openai/openai_adapter.py
+++ b/src/mlx_omni_server/chat/openai/openai_adapter.py
@@ -165,8 +165,9 @@ class OpenAIAdapter:
                 for tool in request.tools
             ]
 
-        logger.info(f"messages: {messages}")
-        logger.info(f"template_kwargs: {template_kwargs}")
+        # Avoid logging full prompts/kwargs at info level (can be large and may contain secrets).
+        logger.debug(f"messages_count={len(messages)}")
+        logger.debug(f"template_kwargs_keys={sorted(template_kwargs.keys())}")
 
         json_schema = None
         if request.response_format and request.response_format.json_schema:
diff --git a/src/mlx_omni_server/chat/openai/router.py b/src/mlx_omni_server/chat/openai/router.py
index 5881888..9d8e0c8 100644
--- a/src/mlx_omni_server/chat/openai/router.py
+++ b/src/mlx_omni_server/chat/openai/router.py
@@ -1,3 +1,5 @@
+import os
+import threading
 import json
 from typing import Generator, Optional
 
@@ -14,6 +16,29 @@ from mlx_omni_server.chat.openai.schema import (
 router = APIRouter(tags=["chatâ€”completions"])
 
 
+def _prewarm_models_async() -> None:
+    models = os.environ.get("MLX_OMNI_PREWARM_MODELS", "").strip()
+    if not models:
+        return
+
+    def _run() -> None:
+        for model_id in [m.strip() for m in models.split(",") if m.strip()]:
+            try:
+                ChatGenerator.get_or_create(
+                    model_id=model_id,
+                    adapter_path=None,
+                    draft_model_id=None,
+                )
+            except Exception:
+                # Prewarm failures should not prevent the server from serving.
+                pass
+
+    threading.Thread(target=_run, daemon=True).start()
+
+
+_prewarm_models_async()
+
+
 @router.post("/chat/completions", response_model=ChatCompletionResponse)
 @router.post("/v1/chat/completions", response_model=ChatCompletionResponse)
 async def create_chat_completion(request: ChatCompletionRequest):
diff --git a/src/mlx_omni_server/chat/mlx/decoding/__init__.py b/src/mlx_omni_server/chat/mlx/decoding/__init__.py
new file mode 100644
index 0000000..2f65c7c
--- /dev/null
+++ b/src/mlx_omni_server/chat/mlx/decoding/__init__.py
@@ -0,0 +1 @@
+"""Decode-time algorithms that must run in-process with the MLX decode loop."""
diff --git a/src/mlx_omni_server/chat/mlx/decoding/entropy_decoding.py b/src/mlx_omni_server/chat/mlx/decoding/entropy_decoding.py
new file mode 100644
index 0000000..c2c8e0a
--- /dev/null
+++ b/src/mlx_omni_server/chat/mlx/decoding/entropy_decoding.py
@@ -0,0 +1,52 @@
+"""Entropy-based decoding helpers for MLX-LM.
+
+These are decode-time algorithms (not proxy-safe). They must run where the
+decode loop and logits are available.
+"""
+
+from __future__ import annotations
+
+from dataclasses import dataclass
+
+import mlx.core as mx
+
+
+@dataclass
+class EntropyTemperatureLogitsProcessor:
+    """Adapt temperature based on the entropy of the current next-token distribution.
+
+    This processor rescales logits by 1/temp, where temp is mapped from entropy.
+    It composes cleanly with existing samplers like top-k/top-p/min-p.
+    """
+
+    entropy_low: float = 1.5
+    entropy_high: float = 4.0
+    temp_low: float = 0.7
+    temp_high: float = 1.3
+
+    def __call__(self, tokens: mx.array, logits: mx.array) -> mx.array:
+        # logits is typically shape (1, vocab). Keep that convention.
+        if logits.ndim == 1:
+            logits = logits[None, :]
+
+        # logprobs = logits - logsumexp(logits)
+        logprobs = logits - mx.logsumexp(logits, keepdims=True)
+        probs = mx.exp(logprobs)
+        entropy = (-mx.sum(probs * logprobs)).item()
+
+        # Map entropy -> temperature (piecewise linear).
+        if self.entropy_high <= self.entropy_low:
+            temp = self.temp_high
+        elif entropy <= self.entropy_low:
+            temp = self.temp_low
+        elif entropy >= self.entropy_high:
+            temp = self.temp_high
+        else:
+            frac = (entropy - self.entropy_low) / (self.entropy_high - self.entropy_low)
+            temp = self.temp_low + frac * (self.temp_high - self.temp_low)
+
+        # Safety clamp.
+        if temp <= 1e-6:
+            temp = 1e-6
+
+        return logits / temp
diff --git a/src/mlx_omni_server/chat/mlx/logprobs_processor.py b/src/mlx_omni_server/chat/mlx/logprobs_processor.py
index acb44b0..d7877f5 100644
--- a/src/mlx_omni_server/chat/mlx/logprobs_processor.py
+++ b/src/mlx_omni_server/chat/mlx/logprobs_processor.py
@@ -47,13 +47,13 @@ class LogprobsProcessor:
         }
 
         top_logprobs = []
-        if top_k is not None:
+        if top_k is not None and top_k > 0:
             top_indices = mx.argpartition(-current_logprobs, kth=top_k - 1)[:top_k]
             top_probs = mx.clip(current_logprobs[top_indices], a_min=-100, a_max=None)
 
             for idx, logprob in zip(top_indices.tolist(), top_probs.tolist()):
                 token = self.tokenizer.decode([idx])
                 token_bytes = token.encode("utf-8")
                 top_logprobs.append(
                     {"token": token, "logprob": logprob, "bytes": list(token_bytes)}
                 )
diff --git a/src/mlx_omni_server/chat/mlx/chat_generator.py b/src/mlx_omni_server/chat/mlx/chat_generator.py
index b44ddcc..daed19d 100644
--- a/src/mlx_omni_server/chat/mlx/chat_generator.py
+++ b/src/mlx_omni_server/chat/mlx/chat_generator.py
@@ -319,6 +319,7 @@ class ChatGenerator:
             final_stream_result = None
             all_text_tokens = []
             all_reasoning_tokens = []
+            content_logprobs = []
 
             for stream_result in self.generate_stream(
                 messages,
@@ -331,13 +332,16 @@ class ChatGenerator:
                 enable_prompt_cache,
                 **kwargs,
             ):
                 # Collect deltas to reconstruct complete content
-                if stream_result.content.text_delta:
-                    complete_raw_text += stream_result.content.text_delta
+                if stream_result.content.text_delta is not None:
+                    delta = stream_result.content.text_delta
+                    complete_raw_text += delta
                     all_text_tokens.append(stream_result.content.token)
-                elif stream_result.content.reasoning_delta:
+                    if stream_result.logprobs is not None:
+                        content_logprobs.append(stream_result.logprobs)
+                elif stream_result.content.reasoning_delta is not None:
                     # Reasoning tokens should also be included in complete_raw_text
-                    complete_raw_text += stream_result.content.reasoning_delta
+                    complete_raw_text += stream_result.content.reasoning_delta
                     all_reasoning_tokens.append(stream_result.content.token)
 
                 final_stream_result = stream_result
@@ -362,11 +366,17 @@ class ChatGenerator:
                 reasoning_tokens=all_reasoning_tokens if all_reasoning_tokens else None,
             )
 
+            # Non-stream OpenAI responses expect a list of per-token logprobs.
+            # We only include tokens that were surfaced as "content" (not reasoning deltas).
+            final_logprobs = None
+            if content_logprobs:
+                final_logprobs = {"content": content_logprobs}
+
             # Return final result with all processing applied
             return GenerationResult(
                 content=content,
                 finish_reason=finish_reason,
                 stats=final_stream_result.stats,
-                logprobs=final_stream_result.logprobs,
+                logprobs=final_logprobs,
                 from_draft=final_stream_result.from_draft,
             )
@@ -476,15 +485,15 @@ class ChatGenerator:
                 chunk_index = len(generated_tokens)
 
                 # Determine which delta field to populate
-                if parse_result.thinking:
+                if parse_result.thinking is not None:
                     content = StreamContent(
                         reasoning_delta=parse_result.thinking,
                         token=response.token,
                         chunk_index=chunk_index,
                     )
                 else:
                     content = StreamContent(
                         text_delta=parse_result.content or response.text,
                         token=response.token,
                         chunk_index=chunk_index,
                     )
diff --git a/src/mlx_omni_server/chat/openai/openai_adapter.py b/src/mlx_omni_server/chat/openai/openai_adapter.py
index 668b54f..aa8d03b 100644
--- a/src/mlx_omni_server/chat/openai/openai_adapter.py
+++ b/src/mlx_omni_server/chat/openai/openai_adapter.py
@@ -119,8 +119,10 @@ class OpenAIAdapter:
         # Extract parameters from request and extra params
         extra_params = request.get_extra_params()
         extra_body = extra_params.get("extra_body", {})
+        if extra_body is None:
+            extra_body = {}
 
         # Prepare sampler configuration
         sampler_config = {
             "temp": 1.0 if request.temperature is None else request.temperature,
             "top_p": 1.0 if request.top_p is None else request.top_p,
@@ -139,9 +141,42 @@ class OpenAIAdapter:
             sampler_config["xtc_threshold"] = extra_body.get("xtc_threshold")
 
         # Prepare template parameters - include both extra_body and direct extra params
         template_kwargs = dict(extra_body)
+        decoding = None
+        if isinstance(extra_body, dict):
+            decoding = extra_body.get("decoding")
+        if decoding is None and "decoding" in extra_params:
+            decoding = extra_params.get("decoding")
+
+        # Don't leak decoding control keys into the model template variables.
+        template_kwargs.pop("decoding", None)
+
+        logits_processors = []
+        if decoding == "entropy_decoding":
+            from mlx_omni_server.chat.mlx.decoding.entropy_decoding import (
+                EntropyTemperatureLogitsProcessor,
+            )
+
+            def _get_float(name: str, default: float) -> float:
+                raw = extra_body.get(name, default)
+                try:
+                    return float(raw)
+                except (TypeError, ValueError):
+                    return default
+
+            logits_processors.append(
+                EntropyTemperatureLogitsProcessor(
+                    entropy_low=_get_float("entropy_low", 1.5),
+                    entropy_high=_get_float("entropy_high", 4.0),
+                    temp_low=_get_float("temp_low", 0.7),
+                    temp_high=_get_float("temp_high", 1.3),
+                )
+            )
+        elif decoding == "thinkdeeper":
+            # v1: map to the existing in-process thinking parser path.
+            template_kwargs["enable_thinking"] = True
 
         # Handle direct extra parameters (for backward compatibility)
         for key in ["enable_thinking"]:
             if key in extra_params:
                 template_kwargs[key] = extra_params[key]
@@ -192,9 +230,14 @@ class OpenAIAdapter:
             "tools": tools,
             "max_tokens": max_tokens,
             "sampler": sampler_config,
-            "top_logprobs": request.top_logprobs if request.logprobs else None,
+            "top_logprobs": (
+                (request.top_logprobs if request.top_logprobs is not None else 0)
+                if request.logprobs
+                else None
+            ),
             "template_kwargs": template_kwargs,
             "enable_prompt_cache": True,
             "repetition_penalty": request.presence_penalty,
             "json_schema": json_schema,
+            **({"logits_processors": logits_processors} if logits_processors else {}),
         }
@@ -272,6 +316,7 @@ class OpenAIAdapter:
         chat_id: str,
         model: str,
         content: Optional[str] = None,
+        reasoning: Optional[str] = None,
         tool_calls: Optional[List[ToolCall]] = None,
         finish_reason: Optional[str] = None,
         logprobs: Optional[Any] = None,
@@ -284,15 +329,18 @@ class OpenAIAdapter:
             model: The model name
             content: Text content for the delta (mutually exclusive with tool_calls)
+            reasoning: Reasoning content for the delta (mutually exclusive with tool_calls/content)
             tool_calls: Tool calls for the delta (mutually exclusive with content)
             finish_reason: The finish reason (None for intermediate chunks)
             logprobs: Log probabilities if requested
 
         Returns:
             ChatCompletionChunk ready to yield
         """
         if tool_calls:
             delta = ChatMessage(role=Role.ASSISTANT, tool_calls=tool_calls)
-        elif content:
+        elif reasoning is not None:
+            delta = ChatMessage(role=Role.ASSISTANT, reasoning=reasoning)
+        elif content is not None:
             delta = ChatMessage(role=Role.ASSISTANT, content=content)
         else:
             delta = ChatMessage(role=Role.ASSISTANT)
@@ -416,29 +465,36 @@ class OpenAIAdapter:
             # Stream content chunks
             for chunk in self._generate_wrapper.generate_stream(**params):
                 # Extract content from chunk
-                if chunk.content.text_delta:
-                    content = chunk.content.text_delta
-                    accumulated_text += content
-                elif chunk.content.reasoning_delta:
-                    content = chunk.content.reasoning_delta
-                    accumulated_text += content
-                else:
-                    content = ""
+                content = None
+                reasoning = None
+                if chunk.content.text_delta is not None:
+                    content = chunk.content.text_delta
+                    accumulated_text += content
+                elif chunk.content.reasoning_delta is not None:
+                    reasoning = chunk.content.reasoning_delta
+                    accumulated_text += reasoning
 
                 # Filter tool call XML when tools are available
-                if has_tools and content:
+                if has_tools and content:
                     stream_content, pending_buffer, in_tool_call = self._filter_stream_content(
                         content, pending_buffer, in_tool_call
                     )
                 else:
-                    stream_content = content
+                    stream_content = content or ""
 
                 # Yield content chunk
-                if stream_content:
+                if reasoning:
+                    yield self._create_stream_chunk(
+                        chat_id,
+                        request.model,
+                        reasoning=reasoning,
+                        logprobs=chunk.logprobs,
+                    )
+                elif stream_content:
                     yield self._create_stream_chunk(
                         chat_id,
                         request.model,
                         content=stream_content,
                         logprobs=chunk.logprobs,
                     )
                 result = chunk
