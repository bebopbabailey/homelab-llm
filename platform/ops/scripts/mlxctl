#!/usr/bin/env python3
import argparse
import json
import os
import re
import shlex
import socket
import subprocess
import sys
import time
import shutil
import tempfile
from contextlib import contextmanager
from pathlib import Path
from typing import Optional
from urllib import request as urlrequest

try:
    import fcntl
except ImportError:  # pragma: no cover - non-POSIX platforms
    fcntl = None


DEFAULT_REGISTRY = "/Users/thestudio/models/hf/hub/registry.json"
DEFAULT_HF_HOME = "/Users/thestudio/models/hf"
DEFAULT_OFFLOAD_SCRIPT = "/home/christopherbailey/homelab-llm/logs/mlx_offload.sh"
DEFAULT_CONVERT_CMD = "python3 -m mlx_lm.convert --hf-path {src} --mlx-path {dest}"
DEFAULT_GATEWAY_ROOT = "/home/christopherbailey/homelab-llm"
DEFAULT_GATEWAY_HOST = "mini"
DEFAULT_GATEWAY_MLXCTL = "/home/christopherbailey/homelab-llm/platform/ops/scripts/mlxctl"
DEFAULT_HANDLES_PATH = "/home/christopherbailey/homelab-llm/layer-gateway/registry/handles.jsonl"
DEFAULT_ENSEMBLES_DOC = "/home/christopherbailey/homelab-llm/layer-gateway/optillm-proxy/ENSEMBLES.md"
DEFAULT_MLX_HOST = "192.168.1.72"
DEFAULT_CONTEXT_LENGTH = 131072
DEFAULT_MAX_OUTPUT_TOKENS = 64000
DEFAULT_TEMPLATE_DIR = "/opt/mlx-launch/templates"

TEAM_RANGE = range(8100, 8120)
EXPERIMENTAL_RANGE = range(8120, 8140)
PORT_RANGE = range(8100, 8140)

PORT_MAP = {}

ENSEMBLES = {}

# MLX Omni Server (canary + future cutover)
DEFAULT_OMNI_REPO = "https://github.com/madroidmaq/mlx-omni-server.git"
DEFAULT_OMNI_REF = "adc8dd2025f6574f6ccebaa7b4bb6a6d735a9780"  # v0.5.2
DEFAULT_OMNI_DIR = "/opt/mlx-omni-launch"
DEFAULT_OMNI_LABEL_PREFIX = "com.bebop.mlx-omni"
DEFAULT_MLX_OPENAI_LAUNCHD_LABEL = "com.bebop.mlx-launch"


def _run(cmd, *, input_bytes=None, check=True):
    result = subprocess.run(
        cmd,
        input=input_bytes,
        capture_output=True,
        text=False,
        check=False,
    )
    if check and result.returncode != 0:
        stderr = (result.stderr or b"").decode("utf-8", errors="replace")
        raise SystemExit(f"command failed (exit {result.returncode}): {' '.join(cmd)}\n{stderr}")
    return result


def _ssh_cmd(studio_ssh: str, argv, *, input_bytes=None, check=True):
    # IMPORTANT: `ssh host cmd arg...` is re-serialized into a single command line on
    # the remote side, so argument boundaries are not preserved. Always send a single
    # quoted command string to be executed by `bash -lc`.
    if isinstance(argv, str):
        remote = argv
    else:
        remote = " ".join(shlex.quote(part) for part in argv)
    cmd = "bash -lc " + shlex.quote(remote)
    return _run(["ssh", studio_ssh, cmd], input_bytes=input_bytes, check=check)


def _studio_sudo_cmd(studio_ssh: str, argv, *, input_bytes=None, check=True):
    # Same ssh boundary issue as _ssh_cmd: build a single quoted remote command string.
    if isinstance(argv, str):
        remote = argv
    else:
        remote = " ".join(shlex.quote(part) for part in argv)
    cmd = "sudo -n bash -lc " + shlex.quote(remote)
    return _run(["ssh", studio_ssh, cmd], input_bytes=input_bytes, check=check)


def _read_patch_bytes(path: Path) -> bytes:
    if not path.exists():
        raise SystemExit(f"missing patch file: {path}")
    return path.read_bytes()


def _is_local_host():
    host = socket.gethostname().split(".")[0].lower()
    return host in {"studio", "thestudio"}


def _maybe_forward_to_studio(args):
    if args.local:
        return
    if getattr(args, "command", None) == "offload-og":
        return
    if getattr(args, "command", None) == "sync-gateway":
        return
    # Omni + cutover ops orchestrate Studio actions from the caller host.
    if getattr(args, "command", None) in {"omni-install", "omni-stop", "omni-status", "mlx-launch-stop", "mlx-launch-start"}:
        return
    if getattr(args, "command", None) == "assign-team":
        return
    if _is_local_host():
        return
    studio_host = os.environ.get("STUDIO_HOST", "studio")
    studio_cmd = os.environ.get("STUDIO_MLXCTL", "/Users/thestudio/bin/mlxctl")
    cmd = ["ssh", studio_host, studio_cmd, "--local"] + sys.argv[1:]
    raise SystemExit(subprocess.call(cmd))


def _registry_path():
    return Path(os.environ.get("MLX_REGISTRY_PATH", DEFAULT_REGISTRY))


def _hf_home():
    return Path(os.environ.get("HF_HOME", DEFAULT_HF_HOME))


@contextmanager
def _registry_lock(path: Path):
    lock_path = path.with_suffix(".lock")
    lock_path.parent.mkdir(parents=True, exist_ok=True)
    with lock_path.open("w") as lock_handle:
        if fcntl:
            fcntl.flock(lock_handle.fileno(), fcntl.LOCK_EX)
        yield
        if fcntl:
            fcntl.flock(lock_handle.fileno(), fcntl.LOCK_UN)


def _load_registry(path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    if not path.exists():
        path.write_text(json.dumps({"version": 1, "models": {}}, indent=2))
    with _registry_lock(path):
        data = json.loads(path.read_text() or "{}")
    data.setdefault("version", 1)
    data.setdefault("models", {})
    return data


def _port_for_model(model_id: str, data):
    entry = data.get("models", {}).get(model_id)
    if entry and entry.get("port"):
        return int(entry["port"])
    for slug, row in data.get("models", {}).items():
        if row.get("model_id") == model_id and row.get("port"):
            return int(row["port"])
    return PORT_MAP.get(model_id)


def _load_ensembles_doc(path: Path):
    if not path.exists():
        return {}
    handle = None
    ensembles = {}
    for raw in path.read_text().splitlines():
        line = raw.strip()
        match = re.match(r"- \*\*(opt-[^*]+)\*\*", line)
        if match:
            handle = match.group(1)
            ensembles.setdefault(handle, [])
            continue
        if not handle:
            continue
        if line.startswith("- Base:") or line.startswith("- Bases:"):
            models = re.findall(r"`([^`]+)`", line)
            if models:
                ensembles[handle].extend(models)
    return ensembles


def _write_registry(path: Path, data):
    tmp = path.with_suffix(".tmp")
    payload = json.dumps(data, indent=2)
    with _registry_lock(path):
        tmp.write_text(payload)
        tmp.replace(path)


def _slugify(text: str):
    text = text.lower().replace("/", "-")
    text = re.sub(r"[^a-z0-9-_]+", "-", text).strip("-")
    return text


def _is_quant_token(token: str) -> bool:
    if token in {
        "mxfp4",
        "mxfp8",
        "fp16",
        "fp8",
        "fp4",
        "int4",
        "int8",
        "int16",
        "bf16",
    }:
        return True
    if re.match(r"^q\\d+$", token):
        return True
    if token.endswith("bit"):
        return True
    return False


def _extract_param_token(tokens):
    for idx, token in enumerate(tokens):
        if token.endswith("b") and token[0].isdigit():
            if idx > 0 and tokens[idx - 1].isdigit():
                return idx - 1, idx, f"{tokens[idx - 1]}-{token}"
            return idx, idx, token
    return None


def _canonical_model_id(repo_id: str):
    _org, _sep, name = repo_id.partition("/")
    base = _slugify(name)
    if base.startswith("mlx-"):
        base = base[len("mlx-"):]
    base = base.replace("-mlx-", "-")
    if base.endswith("-mlx"):
        base = base[:-4]
    tokens = [t for t in base.split("-") if t]
    if not tokens:
        return "mlx-unknown"

    param_info = _extract_param_token(tokens)
    if not param_info:
        return f"mlx-{base}"

    param_start, param_end, param_token = param_info
    family_tokens = tokens[:param_start]
    remainder = tokens[param_end + 1 :]
    quant_tokens = [t for t in remainder if _is_quant_token(t)]
    other_tokens = [t for t in remainder if t not in quant_tokens]
    ordered = family_tokens + [param_token] + quant_tokens + other_tokens
    return "mlx-" + "-".join(ordered)


def _repo_from_dir(name: str):
    if not name.startswith("models--"):
        return None
    parts = name.replace("models--", "", 1).split("--", 1)
    if len(parts) != 2:
        return None
    return f"{parts[0]}/{parts[1]}"


def _latest_snapshot(model_dir: Path):
    snapshots = model_dir / "snapshots"
    if not snapshots.exists():
        return None
    candidates = list(snapshots.iterdir())
    if not candidates:
        return None
    return max(candidates, key=lambda p: p.stat().st_mtime)


def _guess_format(repo_id: str):
    return "gguf" if "gguf" in repo_id.lower() else "mlx"


def _find_entry(data, model_ref: str):
    models = data.get("models", {})
    if model_ref in models:
        return model_ref, models[model_ref]
    for slug, entry in models.items():
        if entry.get("repo_id") == model_ref:
            return slug, entry
    return None, None


def _is_preconverted(repo_id: str, override: Optional[str] = None):
    if override == "yes":
        return True
    if override == "no":
        return False
    org, _sep, _name = repo_id.partition("/")
    orgs = os.environ.get("MLX_PRECONVERTED_ORGS", "mlx-community,mlx,ml-explore").split(",")
    orgs = {o.strip().lower() for o in orgs if o.strip()}
    hints = os.environ.get("MLX_PRECONVERTED_HINTS", "mlx").split(",")
    hints = {h.strip().lower() for h in hints if h.strip()}
    repo_lower = repo_id.lower()
    return org.lower() in orgs or any(hint in repo_lower for hint in hints)


def _port_listening(port: int):
    try:
        result = subprocess.run(
            ["lsof", "-tiTCP:%d" % port, "-sTCP:LISTEN"],
            capture_output=True,
            text=True,
            check=False,
        )
    except FileNotFoundError:
        raise SystemExit("lsof is required on the Studio to detect ports.")
    pids = [line for line in result.stdout.splitlines() if line.strip()]
    return pids


def _ps_snapshot():
    try:
        result = subprocess.run(
            ["ps", "-ax", "-o", "pid=,ppid=,command="],
            capture_output=True,
            text=True,
            check=False,
        )
    except FileNotFoundError:
        raise SystemExit("ps is required on the Studio to inspect processes.")
    rows = []
    for line in result.stdout.splitlines():
        line = line.strip()
        if not line:
            continue
        parts = line.split(None, 2)
        if len(parts) < 3:
            continue
        pid, ppid, cmd = parts
        rows.append((int(pid), int(ppid), cmd))
    return rows


def _mlx_pids_for_port(port: int):
    port_token = re.compile(rf"--port(?:=|\s+){port}\b")
    pids = []
    for pid, _ppid, cmd in _ps_snapshot():
        if "mlx-openai-server" not in cmd:
            continue
        if port_token.search(cmd):
            pids.append(pid)
    return pids


def _include_children(pids):
    snapshot = _ps_snapshot()
    by_ppid = {}
    for pid, ppid, cmd in snapshot:
        by_ppid.setdefault(ppid, []).append((pid, cmd))
    expanded = set(pids)
    queue = list(pids)
    while queue:
        current = queue.pop()
        for child_pid, _cmd in by_ppid.get(current, []):
            if child_pid not in expanded:
                expanded.add(child_pid)
                queue.append(child_pid)
    return sorted(expanded)


def _kill_pids(pids, timeout: int = 10):
    if not pids:
        return
    for pid in pids:
        subprocess.run(["kill", str(pid)], check=False)
    deadline = time.time() + timeout
    remaining = set(pids)
    while remaining and time.time() < deadline:
        live = set(pid for pid, _ppid, _cmd in _ps_snapshot())
        remaining = remaining.intersection(live)
        if remaining:
            time.sleep(0.5)
    if remaining:
        for pid in remaining:
            subprocess.run(["kill", "-9", str(pid)], check=False)


def _stop_mlx_for_port(port: int):
    pids = _mlx_pids_for_port(port)
    if not pids:
        return
    tree = _include_children(pids)
    _kill_pids(tree)


def _launchd_loaded(label: str):
    if sys.platform != "darwin":
        return False
    result = subprocess.run(
        ["launchctl", "print", f"system/{label}"],
        capture_output=True,
        text=True,
        check=False,
    )
    return result.returncode == 0


def _maybe_stop_launchd(label: str):
    if not _launchd_loaded(label):
        return
    if os.geteuid() != 0:
        print(f"launchd job {label} is loaded; stop it with:")
        print(f"sudo launchctl bootout system/{label}")
        return
    subprocess.run(["launchctl", "bootout", f"system/{label}"], check=False)


def _prompt_yes_no(message: str):
    reply = input(f"{message} [y/N]: ").strip().lower()
    return reply in {"y", "yes"}


def _model_on_port(data, port: int):
    for slug, entry in data["models"].items():
        if entry.get("port") == port:
            return slug, entry
    return None, None


def _wait_for_port(port: int, timeout: int = 60):
    start = time.time()
    while time.time() - start < timeout:
        if _port_listening(port):
            return True
        time.sleep(1)
    return False


def _available_port(data, preferred_range):
    assigned = {entry.get("port") for entry in data.get("models", {}).values() if entry.get("port")}
    for port in preferred_range:
        if port in assigned:
            continue
        if _port_listening(port) or _mlx_pids_for_port(port):
            continue
        return port
    return None


def cmd_init(_args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    models = data["models"]

    hub = _hf_home() / "hub"
    if not hub.exists():
        raise SystemExit(f"HF hub directory not found: {hub}")

    added = 0
    for model_dir in hub.iterdir():
        repo_id = _repo_from_dir(model_dir.name)
        if not repo_id:
            continue
        slug = _canonical_model_id(repo_id)
        if slug in models:
            _ensure_defaults(models[slug])
            continue
        snapshot = _latest_snapshot(model_dir)
        if not snapshot:
            continue
        models[slug] = {
            "repo_id": repo_id,
            "model_id": slug,
            "cache_path": str(snapshot),
            "format": _guess_format(repo_id),
            "port": None,
        }
        _ensure_defaults(models[slug])
        added += 1

    _write_registry(registry_path, data)
    print(f"registry initialized: {registry_path} (added {added})")


def cmd_list(_args):
    data = _load_registry(_registry_path())
    for slug, entry in sorted(data["models"].items()):
        port = entry.get("port")
        repo_id = entry.get("repo_id")
        ctx = entry.get("context_length") or "-"
        max_out = entry.get("max_output_tokens") or "-"
        print(f"{slug}\t{repo_id}\t{port if port else '-'}\t{ctx}\t{max_out}")


def cmd_status(_args):
    data = _load_registry(_registry_path())
    port_map = {entry.get("port"): slug for slug, entry in data["models"].items() if entry.get("port")}
    for port in PORT_RANGE:
        pids = _port_listening(port)
        proc_pids = _mlx_pids_for_port(port)
        slug = port_map.get(port)
        status = "listening" if pids else ("starting" if proc_pids else "idle")
        pid_str = ",".join(pids if pids else [str(pid) for pid in proc_pids]) if (pids or proc_pids) else "-"
        model_str = slug if slug else "-"
        print(f"{port}\t{status}\t{pid_str}\t{model_str}")


def _infer_context_length(entry: dict) -> Optional[int]:
    cache_path = entry.get("cache_path")
    if not cache_path:
        return None
    path = Path(cache_path)
    candidates = [path / "config.json", path.parent / "config.json"]
    for cfg in candidates:
        if not cfg.exists():
            continue
        try:
            data = json.loads(cfg.read_text())
        except Exception:
            continue
        for key in ("max_position_embeddings", "model_max_length", "n_positions", "max_sequence_length"):
            val = data.get(key)
            if isinstance(val, int) and val > 0 and val < 1_000_000:
                return val
    return None


def _entry_identity_text(entry: dict) -> str:
    parts = [
        str(entry.get("model_id") or ""),
        str(entry.get("repo_id") or ""),
        str(entry.get("cache_path") or ""),
    ]
    return " ".join(parts).lower()


def _is_gpt_oss_entry(entry: dict) -> bool:
    return "gpt-oss" in _entry_identity_text(entry)


def _is_qwen3_entry(entry: dict) -> bool:
    text = _entry_identity_text(entry)
    return "qwen3" in text or "qwen-3" in text


def _resolve_gpt_oss_chat_template(entry: dict) -> Optional[str]:
    text = _entry_identity_text(entry)
    tokens = {tok for tok in re.split(r"[^a-z0-9]+", text) if tok}
    template_dir = Path(os.environ.get("MLX_TEMPLATE_DIR", DEFAULT_TEMPLATE_DIR))
    if not template_dir.exists():
        return None
    candidates = []
    if "120b" in tokens:
        candidates.append("gpt-oss-120b-chat_template.jinja")
    if "20b" in tokens:
        candidates.append("gpt-oss-20b-chat_template.jinja")
    candidates.extend(
        [
            "gpt-oss-chat_template.jinja",
            "gpt-oss-120b-chat_template.jinja",
            "gpt-oss-20b-chat_template.jinja",
        ]
    )
    for name in candidates:
        candidate = template_dir / name
        if candidate.exists():
            return str(candidate)
    return None


def _resolve_local_chat_template(entry: dict) -> Optional[str]:
    cache_path = entry.get("cache_path")
    if not cache_path:
        return None
    path = Path(cache_path)
    candidates = [path / "chat_template.jinja", path.parent / "chat_template.jinja"]
    for candidate in candidates:
        if candidate.exists():
            return str(candidate)
    return None


def _ensure_defaults(entry: dict):
    changed = False
    if not entry.get("context_length"):
        inferred = _infer_context_length(entry)
        entry["context_length"] = inferred or DEFAULT_CONTEXT_LENGTH
        changed = True
    if not entry.get("max_output_tokens"):
        entry["max_output_tokens"] = min(DEFAULT_MAX_OUTPUT_TOKENS, int(entry["context_length"]))
        changed = True

    # Family-safe defaults: GPT-OSS models require Harmony parsing/template.
    if _is_gpt_oss_entry(entry):
        if not entry.get("tool_call_parser"):
            entry["tool_call_parser"] = "harmony"
            changed = True
        if not entry.get("reasoning_parser"):
            entry["reasoning_parser"] = "harmony"
            changed = True
        template = _resolve_gpt_oss_chat_template(entry)
        current_template = entry.get("chat_template")
        if template and not current_template:
            entry["chat_template"] = template
            changed = True
        elif template and current_template:
            tokens = {tok for tok in re.split(r"[^a-z0-9]+", _entry_identity_text(entry)) if tok}
            current_name = Path(current_template).name
            if ("120b" in tokens and "20b" in current_name) or ("20b" in tokens and "120b" in current_name):
                entry["chat_template"] = template
                changed = True
    elif _is_qwen3_entry(entry):
        if not entry.get("tool_call_parser"):
            entry["tool_call_parser"] = "qwen3"
            changed = True
        if not entry.get("reasoning_parser"):
            entry["reasoning_parser"] = "qwen3"
            changed = True
        if not entry.get("chat_template"):
            local_template = _resolve_local_chat_template(entry)
            if local_template:
                entry["chat_template"] = local_template
                changed = True
    return changed


def _raw_harmony_tags_detected(port: int, model_name: str) -> bool:
    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": "Return one short sentence about oranges."}],
        "max_tokens": 128,
    }
    try:
        req = urlrequest.Request(
            f"http://127.0.0.1:{port}/v1/chat/completions",
            data=json.dumps(payload).encode("utf-8"),
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        with urlrequest.urlopen(req, timeout=20) as resp:
            body = resp.read().decode("utf-8")
        data = json.loads(body)
    except Exception:
        return False
    content = data.get("choices", [{}])[0].get("message", {}).get("content")
    return isinstance(content, str) and "<|channel|>" in content


def _raw_qwen3_tags_detected(port: int, model_name: str) -> bool:
    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": "Return one short sentence about apples."}],
        "max_tokens": 96,
    }
    try:
        req = urlrequest.Request(
            f"http://127.0.0.1:{port}/v1/chat/completions",
            data=json.dumps(payload).encode("utf-8"),
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        with urlrequest.urlopen(req, timeout=20) as resp:
            body = resp.read().decode("utf-8")
        data = json.loads(body)
    except Exception:
        return False
    content = data.get("choices", [{}])[0].get("message", {}).get("content")
    if not isinstance(content, str):
        return False
    return any(marker in content for marker in ("<think>", "<tool_call>", "<|im_start|>", "<|im_end|>"))


def _mlx_command_for_port(port: int) -> Optional[str]:
    port_token = re.compile(rf"--port(?:=|\s+){port}\b")
    for _pid, _ppid, cmd in _ps_snapshot():
        if "mlx-openai-server" not in cmd:
            continue
        if port_token.search(cmd):
            return cmd
    return None


def _command_arg_value(command: str, flag: str) -> Optional[str]:
    try:
        parts = shlex.split(command)
    except ValueError:
        return None
    for idx, token in enumerate(parts):
        if token == flag and idx + 1 < len(parts):
            return parts[idx + 1]
        if token.startswith(flag + "="):
            return token.split("=", 1)[1]
    return None


def _download_and_register(data, model_ref: str):
    try:
        from huggingface_hub import snapshot_download
    except ImportError as exc:
        raise SystemExit("huggingface_hub is required for downloads. Install it first.") from exc

    cache_dir = _hf_home()
    repo_id = model_ref
    snapshot = snapshot_download(repo_id, cache_dir=str(cache_dir))
    slug = _canonical_model_id(repo_id)
    data["models"][slug] = {
        "repo_id": repo_id,
        "model_id": slug,
        "cache_path": str(snapshot),
        "format": _guess_format(repo_id),
        "port": None,
    }
    return slug, data["models"][slug]


def _run_convert(repo_id: str, src_path: Path, dest_path: Path):
    cmd_tmpl = os.environ.get("MLX_CONVERT_CMD", DEFAULT_CONVERT_CMD)
    cmd = cmd_tmpl.format(repo=repo_id, src=src_path, dest=dest_path)
    argv = shlex.split(cmd)
    if not argv:
        raise SystemExit("MLX_CONVERT_CMD is empty")
    dest_path.mkdir(parents=True, exist_ok=True)
    result = subprocess.run(argv, check=False)
    if result.returncode != 0:
        raise SystemExit(f"conversion failed (exit {result.returncode})")


def _ensure_model(data, model_ref: str, name: Optional[str], convert_mode: str, preconverted: Optional[str]):
    slug, entry = _find_entry(data, model_ref)
    if entry:
        return slug, entry, False

    try:
        from huggingface_hub import snapshot_download
    except ImportError as exc:
        raise SystemExit("huggingface_hub is required for downloads. Install it first.") from exc

    cache_dir = _hf_home()
    repo_id = model_ref
    snapshot = snapshot_download(repo_id, cache_dir=str(cache_dir))
    slug = name or _canonical_model_id(repo_id)

    preconverted = _is_preconverted(repo_id, preconverted)
    do_convert = convert_mode == "force" or (convert_mode == "auto" and not preconverted)
    if convert_mode == "skip":
        do_convert = False

    repo_root = Path(snapshot).parent.parent
    entry = {
        "repo_id": repo_id,
        "model_id": slug,
        "cache_path": str(snapshot),
        "format": _guess_format(repo_id),
        "port": None,
    }

    if do_convert:
        hub = _hf_home() / "hub"
        dest = hub / f"converted--{slug}"
        if not dest.exists() or not any(dest.iterdir()):
            _run_convert(repo_id, Path(snapshot), dest)
        entry["og_path"] = str(repo_root)
        entry["cache_path"] = str(dest)
        entry["format"] = "mlx"

    data["models"][slug] = entry
    return slug, entry, do_convert


def _launch_server(entry, port: int):
    model_path = entry["cache_path"]
    cmd = os.environ.get("MLX_LAUNCH_CMD")
    if cmd:
        argv = cmd.split()
    elif shutil.which("mlx-openai-server"):
        argv = ["mlx-openai-server", "launch"]
    elif shutil.which("uv"):
        argv = ["uv", "run", "--project", "/opt/mlx-launch", "mlx-openai-server", "launch"]
    elif Path("/opt/homebrew/bin/uv").exists():
        argv = ["/opt/homebrew/bin/uv", "run", "--project", "/opt/mlx-launch", "mlx-openai-server", "launch"]
    else:
        raise SystemExit("mlx-openai-server not found; set MLX_LAUNCH_CMD or install it.")

    argv += [
        "--model-path",
        model_path,
        "--model-type",
        "lm",
        "--trust-remote-code",
        "--host",
        "0.0.0.0",
        "--port",
        str(port),
    ]

    tool_call_parser = entry.get("tool_call_parser") or os.environ.get("MLX_TOOL_CALL_PARSER")
    if tool_call_parser:
        argv += ["--tool-call-parser", tool_call_parser]

    reasoning_parser = entry.get("reasoning_parser") or os.environ.get("MLX_REASONING_PARSER")
    if reasoning_parser:
        argv += ["--reasoning-parser", reasoning_parser]

    chat_template = entry.get("chat_template") or os.environ.get("MLX_CHAT_TEMPLATE")
    if chat_template:
        argv += ["--chat-template-file", chat_template]

    log_dir = os.environ.get("MLX_LOG_DIR", "/tmp")
    Path(log_dir).mkdir(parents=True, exist_ok=True)
    log_file = Path(log_dir) / f"mlx-{port}.log"
    with log_file.open("a") as handle:
        subprocess.Popen(argv, stdout=handle, stderr=handle)


def cmd_load(args):
    if args.port == "auto":
        registry_path = _registry_path()
        data = _load_registry(registry_path)
        port = _available_port(data, EXPERIMENTAL_RANGE)
        if port is None:
            raise SystemExit("no free ports available in experimental range (8120-8139)")
        args.port = str(port)
    port = int(args.port)
    if port not in PORT_RANGE:
        raise SystemExit("port must be between 8100 and 8139")

    registry_path = _registry_path()
    data = _load_registry(registry_path)
    launchd_label = os.environ.get("MLX_LAUNCHD_LABEL", "com.bebop.mlx-launch")
    if not args.ignore_launchd and _launchd_loaded(launchd_label):
        if _prompt_yes_no(f"launchd job {launchd_label} is loaded. Stop it before loading?"):
            _maybe_stop_launchd(launchd_label)
        else:
            raise SystemExit("load cancelled")

    running_pids = _mlx_pids_for_port(port)
    if running_pids or _port_listening(port):
        if args.force:
            cmd_unload(argparse.Namespace(port=str(port), sync=False))
        else:
            slug, _entry = _model_on_port(data, port)
            model_name = slug if slug else "unknown"
            if not _prompt_yes_no(f"model {model_name} already in port {port}. Unload model?"):
                raise SystemExit("load cancelled")
            cmd_unload(argparse.Namespace(port=str(port), sync=False))

    slug, entry = _find_entry(data, args.model)
    converted = False
    if not entry:
        slug, entry, converted = _ensure_model(data, args.model, args.name, args.convert, args.preconverted)
    _ensure_defaults(entry)

    if entry.get("port") and entry.get("port") != port:
        raise SystemExit(f"model already assigned to port {entry['port']}; unload first")

    _launch_server(entry, port)
    if not _wait_for_port(port):
        raise SystemExit(f"model failed to start on port {port}")

    if _is_gpt_oss_entry(entry):
        model_name = entry.get("model_id") or slug
        if _raw_harmony_tags_detected(port, model_name):
            print(
                "warning: raw Harmony tags detected after load. "
                "check chat_template/tool_call_parser/reasoning_parser settings."
            )
    elif _is_qwen3_entry(entry):
        model_name = entry.get("model_id") or slug
        if _raw_qwen3_tags_detected(port, model_name):
            print(
                "warning: raw Qwen protocol tags detected after load. "
                "check qwen3 parser/template settings."
            )

    entry["port"] = port
    _write_registry(registry_path, data)
    if converted and args.offload_og:
        cmd_offload_og(argparse.Namespace(dry_run=False))
    if args.sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False))
    print(f"loaded {slug} on {port}")


def cmd_unload(args):
    port = int(args.port)
    if port not in PORT_RANGE:
        raise SystemExit("port must be between 8100 and 8139")

    _stop_mlx_for_port(port)

    registry_path = _registry_path()
    data = _load_registry(registry_path)
    for entry in data["models"].values():
        if entry.get("port") == port:
            entry["port"] = None
    _write_registry(registry_path, data)
    if args.sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False))
    print(f"unloaded port {port}")


def cmd_unload_all(args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    running = []
    for port in PORT_RANGE:
        if _port_listening(port) or _mlx_pids_for_port(port):
            slug, _entry = _model_on_port(data, port)
            model_name = slug if slug else "unknown"
            running.append((port, model_name))
    if running:
        print("Currently running models:")
        for port, model_name in running:
            print(f"- {model_name} on {port}")
    else:
        print("No running models detected.")
        return
    if not _prompt_yes_no("Unload all models?"):
        print("unload-all cancelled")
        return
    for port in PORT_RANGE:
        _stop_mlx_for_port(port)
        for entry in data["models"].values():
            if entry.get("port") == port:
                entry["port"] = None
    _write_registry(registry_path, data)
    if args.sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False))
    print("unloaded all ports")


def cmd_reconcile(_args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    changed = 0
    for entry in data["models"].values():
        port = entry.get("port")
        if not port:
            continue
        if not _port_listening(int(port)):
            entry["port"] = None
            changed += 1
    _write_registry(registry_path, data)
    print(f"reconciled registry (cleared {changed} stale ports)")


def cmd_verify(_args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    missing = []
    port_to_models = {}
    for slug, entry in data["models"].items():
        _ensure_defaults(entry)
        if not entry.get("context_length"):
            missing.append(f"{slug}: missing context_length")
        if not entry.get("max_output_tokens"):
            missing.append(f"{slug}: missing max_output_tokens")
        if _is_gpt_oss_entry(entry):
            if entry.get("tool_call_parser") != "harmony":
                missing.append(f"{slug}: tool_call_parser must be 'harmony'")
            if entry.get("reasoning_parser") != "harmony":
                missing.append(f"{slug}: reasoning_parser must be 'harmony'")
            template = entry.get("chat_template")
            if not template:
                missing.append(f"{slug}: missing chat_template")
            elif not Path(template).exists():
                missing.append(f"{slug}: chat_template path missing: {template}")
        if _is_qwen3_entry(entry):
            if entry.get("tool_call_parser") != "qwen3":
                missing.append(f"{slug}: tool_call_parser must be 'qwen3'")
            if entry.get("reasoning_parser") != "qwen3":
                missing.append(f"{slug}: reasoning_parser must be 'qwen3'")
            template = entry.get("chat_template")
            if template and not Path(template).exists():
                missing.append(f"{slug}: chat_template path missing: {template}")
        port = entry.get("port")
        if port:
            port_to_models.setdefault(int(port), []).append(slug)
            cmd = _mlx_command_for_port(int(port))
            if not cmd:
                missing.append(f"{slug}: registry port {port} set but no MLX process listening")
            else:
                model_path = _command_arg_value(cmd, "--model-path")
                if model_path and entry.get("cache_path") and model_path != entry.get("cache_path"):
                    missing.append(
                        f"{slug}: runtime path mismatch on {port} "
                        f"(registry={entry.get('cache_path')} runtime={model_path})"
                    )
    for port, slugs in sorted(port_to_models.items()):
        if len(slugs) > 1:
            missing.append(f"port {port}: multiple registry assignments ({', '.join(sorted(slugs))})")

    # Omni-first routing: if we're on the gateway host, validate served MLX handles exist in registry.json.
    handles_path = Path(os.environ.get("HANDLES_PATH", DEFAULT_HANDLES_PATH))
    if handles_path.exists():
        try:
            handles_rows = _read_handles_jsonl(handles_path)
            served = _served_mlx_handles(handles_rows)
            for row in served:
                handle = row["handle"]
                if _registry_entry_for_handle(data.get("models", {}), handle) is None:
                    missing.append(
                        f"handles.jsonl: served handle missing from registry.json: {handle} ({row.get('endpoint_ref')})"
                    )
        except Exception as exc:
            missing.append(f"handles.jsonl: failed to validate served handles: {exc}")
    _write_registry(registry_path, data)
    if missing:
        raise SystemExit("mlxctl verify failed:\n  - " + "\n  - ".join(missing))
    print("mlxctl verify ok: registry defaults and routing state are consistent")


def cmd_ensemble(args):
    raise SystemExit("mlxctl ensemble is deprecated; use load/assign-team instead")


def cmd_ensure(args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    slug, entry, converted = _ensure_model(data, args.repo, args.name, args.convert, args.preconverted)
    _ensure_defaults(entry)
    _write_registry(registry_path, data)
    if converted and args.offload_og:
        cmd_offload_og(argparse.Namespace(dry_run=False))
    if args.load:
        cmd_load(
            argparse.Namespace(
                model=slug,
                port=args.port,
                force=False,
                ignore_launchd=False,
                convert="skip",
                name=None,
                offload_og=False,
                preconverted="auto",
                sync=args.sync,
            )
        )
    elif args.sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False))


def _fetch_registry_from_studio(studio_ssh: str, registry_path: str):
    script = f"""
import json
from pathlib import Path
path = Path({registry_path!r})
if not path.exists():
    raise SystemExit(1)
print(path.read_text())
"""
    result = subprocess.run(
        ["ssh", studio_ssh, "python3", "-"],
        input=script.encode("utf-8"),
        capture_output=True,
        check=False,
    )
    if result.returncode != 0:
        raise SystemExit("failed to read MLX registry from studio")
    return json.loads(result.stdout.decode("utf-8") or "{}")


def _fetch_registry_sizes(studio_ssh: str, registry_path: str):
    script = f"""
import json
import os
from pathlib import Path

path = Path({registry_path!r})
if not path.exists():
    raise SystemExit(1)
data = json.loads(path.read_text() or "{{}}")
models = data.get("models", {{}})

def repo_root(cache_path: str):
    p = Path(cache_path)
    if p.name.startswith("models--") or p.name.startswith("converted--"):
        return p
    for parent in p.parents:
        name = parent.name
        if name.startswith("models--") or name.startswith("converted--"):
            return parent
    return p

def dir_size(root: Path):
    total = 0
    for base, _dirs, files in os.walk(root, followlinks=False):
        for fname in files:
            fpath = os.path.join(base, fname)
            if os.path.islink(fpath):
                continue
            try:
                total += os.path.getsize(fpath)
            except OSError:
                continue
    return total

rows = []
for slug, entry in models.items():
    cache_path = entry.get("cache_path")
    if not cache_path:
        continue
    root = repo_root(cache_path)
    if not root.exists():
        continue
    rows.append({{
        "slug": slug,
        "model_id": entry.get("model_id") or slug,
        "cache_path": str(cache_path),
        "root": str(root),
        "size_bytes": dir_size(root),
        "port": entry.get("port"),
    }})
print(json.dumps(rows))
"""
    result = subprocess.run(
        ["ssh", studio_ssh, "python3", "-"],
        input=script.encode("utf-8"),
        capture_output=True,
        check=False,
    )
    if result.returncode != 0:
        raise SystemExit("failed to read MLX registry sizes from studio")
    return json.loads(result.stdout.decode("utf-8") or "[]")


def _optillm_models_from_handles(path: Path):
    if not path.exists():
        raise SystemExit(f"handles.jsonl not found: {path}")
    models = set()
    for line in path.read_text().splitlines():
        if not line.strip():
            continue
        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue
        handle = obj.get("handle", "")
        if not handle.startswith("opt-"):
            continue
        selector = obj.get("selector") or {}
        model = selector.get("model")
        if not model:
            continue
        for match in re.findall(r"mlx-[a-z0-9-]+", str(model)):
            models.add(match)
    return models


def _apply_port_map(assignments, studio_ssh: str, sync: bool):
    studio_cmd = os.environ.get("STUDIO_MLXCTL", "/Users/thestudio/bin/mlxctl")
    for handle, port in assignments:
        cmd = [
            "ssh",
            studio_ssh,
            studio_cmd,
            "--local",
            "load",
            handle,
            str(port),
            "--force",
            "--ignore-launchd",
            "--no-sync",
        ]
        subprocess.run(cmd, check=False)
    if sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False))


def cmd_assign_team(args):
    studio_ssh = os.environ.get("STUDIO_SSH", "thestudio@192.168.1.72")
    handles_path = Path(os.environ.get("HANDLES_PATH", DEFAULT_HANDLES_PATH))
    registry_path = os.environ.get("MLX_REGISTRY_PATH", DEFAULT_REGISTRY)

    # ensure registry includes all cached models
    subprocess.run(["ssh", studio_ssh, os.environ.get("STUDIO_MLXCTL", "/Users/thestudio/bin/mlxctl"), "--local", "init"], check=False)

    optillm_models = _optillm_models_from_handles(handles_path)
    rows = _fetch_registry_sizes(studio_ssh, registry_path)
    if not rows:
        raise SystemExit("no MLX models found in registry")

    by_handle = {row["model_id"]: row for row in rows}
    opt_rows = [row for handle, row in by_handle.items() if handle in optillm_models]
    other_rows = [row for handle, row in by_handle.items() if handle not in optillm_models]

    opt_rows.sort(key=lambda r: (-r["size_bytes"], r["model_id"]))
    other_rows.sort(key=lambda r: (-r["size_bytes"], r["model_id"]))

    opt_ports = list(range(8100, 8110))
    other_ports = list(range(8110, 8120))

    if len(opt_rows) > len(opt_ports):
        raise SystemExit("too many optillm models for 8100-8109")
    if len(other_rows) > len(other_ports):
        raise SystemExit("too many non-optillm models for 8110-8119")

    assignments = []
    for row, port in zip(opt_rows, opt_ports):
        assignments.append((row["model_id"], port))
    for row, port in zip(other_rows, other_ports):
        assignments.append((row["model_id"], port))

    if args.dry_run:
        print(json.dumps({"count": len(assignments), "assignments": assignments}, indent=2))
        return

    _apply_port_map(assignments, studio_ssh, sync=args.sync)


def _env_key(handle: str, suffix: str):
    key = handle.upper().replace("-", "_")
    return f"MLX_{key}_{suffix}"

_EP_PORT_RE = re.compile(r"^ep_mlx_(?:omni|slot)_(\d+)$")


def _endpoint_ref_port(endpoint_ref: str) -> Optional[int]:
    if not endpoint_ref:
        return None
    m = _EP_PORT_RE.match(endpoint_ref.strip())
    if not m:
        return None
    try:
        return int(m.group(1))
    except ValueError:
        return None


def _read_handles_jsonl(path: Path):
    if not path.exists():
        raise SystemExit(f"handles.jsonl not found: {path}")
    rows = []
    for line in path.read_text().splitlines():
        if not line.strip():
            continue
        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue
        if isinstance(obj, dict):
            rows.append(obj)
    return rows


def _served_mlx_handles(handles_rows):
    """Return handles served by MLX backends.

    Omni-first: treat handles pointing at ep_mlx_omni_<port> as served.
    Also accept legacy ep_mlx_slot_<port> for transition.
    """
    served = []
    for obj in handles_rows:
        handle = obj.get("handle")
        if not handle or not isinstance(handle, str):
            continue
        endpoint_ref = obj.get("endpoint_ref") or ""
        if not isinstance(endpoint_ref, str):
            continue
        if endpoint_ref.startswith("ep_mlx_omni_") or endpoint_ref.startswith("ep_mlx_slot_"):
            served.append(
                {
                    "handle": handle,
                    "endpoint_ref": endpoint_ref,
                    "selector": obj.get("selector") or {},
                }
            )
    return served


def _registry_entry_for_handle(models: dict, handle: str) -> Optional[dict]:
    entry = models.get(handle)
    if entry:
        return entry
    for _slug, e in (models or {}).items():
        if e.get("model_id") == handle:
            return e
    return None


def _build_mlx_entries(models, host: str):
    items = []
    for slug, entry in models.items():
        port = entry.get("port")
        if not port:
            continue
        handle = entry.get("model_id") or slug
        items.append(
            {
                "handle": handle,
                "port": int(port),
                "request_params": entry.get("request_params", {}),
                "context_length": entry.get("context_length"),
                "max_output_tokens": entry.get("max_output_tokens"),
            }
        )
    return sorted(items, key=lambda i: i["port"])



def _yaml_scalar(value):
    if value is None:
        return "null"
    if isinstance(value, bool):
        return "true" if value else "false"
    if isinstance(value, (int, float)):
        return str(value)
    if isinstance(value, str):
        return json.dumps(value)
    raise SystemExit(f"unsupported request_param type: {type(value)}")


def _update_router_yaml(path: Path, mlx_entries):
    if not path.exists():
        raise SystemExit(f"router.yaml not found: {path}")
    lines = path.read_text().splitlines()
    begin = "# BEGIN MLXCTL AUTO"
    end = "# END MLXCTL AUTO"

    # Remove existing MLX entries (legacy or previous sync)
    new_lines = []
    skip = False
    skip_block = False
    for line in lines:
        if line.strip() == begin:
            skip = True
            continue
        if line.strip() == end:
            skip = False
            continue
        if skip:
            continue
        if line.startswith("- model_name: "):
            if line.startswith("- model_name: mlx-"):
                skip_block = True
                continue
            skip_block = False
        if skip_block:
            continue
        new_lines.append(line)

    block = [
        begin,
        "# MLX model entries omitted (aliases-only model_list).",
        "# Canonical model_ids remain in the MLX registry and env.local.",
        end,
    ]

    inserted = False
    output = []
    for line in new_lines:
        output.append(line)
        if line.strip() == "model_list:" and not inserted:
            output.extend(block)
            inserted = True
    if not inserted:
        output = block + new_lines
    path.write_text("\n".join(output) + "\n")


def _update_env_local(path: Path, mlx_entries, host: str):
    if not path.exists():
        raise SystemExit(f"env.local not found: {path}")
    begin = "# BEGIN MLXCTL AUTO"
    end = "# END MLXCTL AUTO"
    lines = path.read_text().splitlines()
    output = []
    skip = False
    for line in lines:
        if line.strip() == begin:
            skip = True
            continue
        if line.strip() == end:
            skip = False
            continue
        if skip:
            continue
        if (
            line.startswith("JERRY_")
            or line.startswith("BENCH_")
            or line.startswith("MLX_")
            or line.startswith("ROUTER_")
            or line.startswith("SWAP_")
        ):
            continue
        output.append(line)

    block = [begin, "# MLX backend base URLs (auto-generated; Omni-first)"]
    single_api_base = os.environ.get("MLX_SINGLE_API_BASE")
    default_omni_port = int(os.environ.get("MLX_OMNI_PORT", "8100"))
    for item in mlx_entries:
        handle = item["handle"]
        endpoint_port = item.get("endpoint_port")
        port = int(endpoint_port) if endpoint_port else default_omni_port
        api_base = single_api_base or f"http://{host}:{port}/v1"
        block.append(f"{_env_key(handle, 'API_BASE')}={api_base}")
    block.append("")
    block.append("# Upstream model IDs (prefix with \"openai/\" for OpenAI-compatible backends)")
    for item in mlx_entries:
        handle = item["handle"]
        block.append(f"{_env_key(handle, 'MODEL')}=openai/{handle}")
    block.append("")
    block.append("# API keys (set to \"dummy\" if the backend does not enforce auth)")
    for item in mlx_entries:
        handle = item["handle"]
        block.append(f"{_env_key(handle, 'API_KEY')}=dummy")
    block.append("")
    block.append("# Swap endpoint defaults (kept aligned with the MLX backend)")
    swap_api_base = single_api_base or f"http://{host}:{default_omni_port}/v1"
    block.append(f"SWAP_API_BASE={swap_api_base}")
    # swap remains a model id passed to the backend; keep it compatible with existing behavior.
    # (The backend should strip the 'openai/' prefix if it does not use OpenAI provider routing.)
    block.append("SWAP_MODEL=openai/mlx-gpt-oss-20b-mxfp4-q4")
    block.append("SWAP_API_KEY=dummy")
    # OptiLLM routing block removed (deprecated).
    block.append(end)

    output.append("")
    output.extend(block)
    path.write_text("\n".join(output).rstrip() + "\n")


def _update_handles(path: Path, mlx_entries):
    if not path.exists():
        raise SystemExit(f"handles.jsonl not found: {path}")
    replace_handles = {item["handle"] for item in mlx_entries}
    rows = []
    for line in path.read_text().splitlines():
        if not line.strip():
            continue
        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            rows.append(line)
            continue
        handle = obj.get("handle", "")
        endpoint = obj.get("endpoint_ref", "")
        # Drop any previously-generated MLX handle entries so we can re-add them deterministically.
        if (
            handle in replace_handles
            or handle.startswith("mlx-")
            or endpoint.startswith("ep_mlx_slot_")
            or endpoint.startswith("ep_mlx_omni_")
            or endpoint.startswith("ep_mlx_")
        ):
            continue
        rows.append(obj)

    single_endpoint_ref = os.environ.get("MLX_SINGLE_ENDPOINT_REF")
    for item in mlx_entries:
        handle = item["handle"]
        port = item["port"]
        endpoint_ref = f"ep_mlx_slot_{port}"
        if single_endpoint_ref:
            endpoint_ref = single_endpoint_ref
        rows.append(
            {
                "handle": handle,
                "kind": "model",
                "invoke": "openai-chat",
                "managed_by": "launchd",
                "endpoint_ref": endpoint_ref,
                "selector": {"model": handle},
            }
        )

    with path.open("w") as handle_out:
        for row in rows:
            if isinstance(row, str):
                handle_out.write(row + "\n")
            else:
                handle_out.write(json.dumps(row, separators=(",", ":")) + "\n")




def cmd_sync_gateway(args):
    gateway_root = Path(os.environ.get("GATEWAY_ROOT", DEFAULT_GATEWAY_ROOT))
    router_path = gateway_root / "layer-gateway/litellm-orch/config/router.yaml"
    env_path = gateway_root / "layer-gateway/litellm-orch/config/env.local"
    handles_path = gateway_root / "layer-gateway/registry/handles.jsonl"

    if not router_path.exists():
        gateway_host = os.environ.get("GATEWAY_HOST", DEFAULT_GATEWAY_HOST)
        gateway_mlxctl = os.environ.get("GATEWAY_MLXCTL", DEFAULT_GATEWAY_MLXCTL)
        remote_cmd = [gateway_mlxctl, "--local", "sync-gateway"]
        if args.dry_run:
            remote_cmd.append("--dry-run")
        subprocess.run(["ssh", gateway_host] + remote_cmd, check=False)
        return

    studio_ssh = os.environ.get("STUDIO_SSH", "thestudio@192.168.1.72")
    registry_path = os.environ.get("MLX_REGISTRY_PATH", DEFAULT_REGISTRY)
    registry = _fetch_registry_from_studio(studio_ssh, registry_path)
    handles_rows = _read_handles_jsonl(handles_path)
    served = _served_mlx_handles(handles_rows)
    models = registry.get("models", {}) or {}

    # Omni-first: the served set is curated via handles.jsonl; do not rely on registry.port.
    mlx_entries = []
    for row in served:
        handle = row["handle"]
        endpoint_ref = row.get("endpoint_ref") or ""
        endpoint_port = _endpoint_ref_port(endpoint_ref)
        reg_entry = _registry_entry_for_handle(models, handle) or {}
        mlx_entries.append(
            {
                "handle": handle,
                "endpoint_ref": endpoint_ref,
                "endpoint_port": endpoint_port,
                "request_params": reg_entry.get("request_params", {}),
                "context_length": reg_entry.get("context_length"),
                "max_output_tokens": reg_entry.get("max_output_tokens"),
            }
        )
    mlx_entries.sort(key=lambda r: ((r.get("endpoint_port") or 0), r.get("handle") or ""))

    if args.dry_run:
        print(json.dumps({"count": len(mlx_entries), "entries": mlx_entries}, indent=2))
        return

    _update_router_yaml(router_path, mlx_entries)
    _update_env_local(env_path, mlx_entries, os.environ.get("MLX_HOST", DEFAULT_MLX_HOST))
    # NOTE: handles.jsonl is treated as a curated served-set source; sync does not rewrite it.


def cmd_omni_install(args):
    """
    Install and run mlx-omni-server on the Studio under /opt/mlx-omni-launch.

    Orchestrated from the caller host (Mini) so we can ship a pinned patch without
    requiring the monorepo to be present on the Studio.
    """
    studio_ssh = os.environ.get("STUDIO_SSH", "studio")
    repo = os.environ.get("MLX_OMNI_REPO", DEFAULT_OMNI_REPO)
    ref = os.environ.get("MLX_OMNI_REF", DEFAULT_OMNI_REF)
    install_dir = os.environ.get("MLX_OMNI_DIR", DEFAULT_OMNI_DIR)
    port = int(args.port)
    bind_host = args.host

    label_prefix = os.environ.get("MLX_OMNI_LABEL_PREFIX", DEFAULT_OMNI_LABEL_PREFIX)
    label = f"{label_prefix}.{port}"

    # Disk check before installing python deps on Studio.
    _ssh_cmd(studio_ssh, "df -h / /Users/thestudio 2>/dev/null || df -h /", check=False)

    # Prepare install dir
    _studio_sudo_cmd(studio_ssh, f"mkdir -p {shlex.quote(install_dir)}")
    _studio_sudo_cmd(studio_ssh, f"chown -R thestudio:staff {shlex.quote(install_dir)}")

    # Clone/update and pin
    remote_setup = f"""
set -euo pipefail
cd {shlex.quote(install_dir)}
if [ ! -d .git ]; then
  git init
  git remote add origin {shlex.quote(repo)}
fi
git fetch --tags --force origin
git checkout -f {shlex.quote(ref)}
git clean -fd
"""
    _ssh_cmd(studio_ssh, remote_setup)

    # Apply patch (best-effort idempotency via --check)
    gateway_root = Path(os.environ.get("GATEWAY_ROOT", DEFAULT_GATEWAY_ROOT))
    patch_path = gateway_root / "platform/ops/patches/mlx-omni-server-v0.5.2.patch"
    patch_bytes = _read_patch_bytes(patch_path)
    remote_apply = f"""
set -euo pipefail
cd {shlex.quote(install_dir)}
tmp_patch="$(mktemp)"
cat > "$tmp_patch"
if git apply --check "$tmp_patch" >/dev/null 2>&1; then
  git apply "$tmp_patch"
fi
rm -f "$tmp_patch"
"""
    _ssh_cmd(studio_ssh, remote_apply, input_bytes=patch_bytes, check=True)

    # Ensure uv exists on Studio
    _ssh_cmd(
        studio_ssh,
        "command -v uv >/dev/null 2>&1 || (echo 'uv not found on Studio (install uv first)' >&2; exit 2)",
    )

    # Install into local venv (editable install to keep patch active)
    remote_install = f"""
set -euo pipefail
cd {shlex.quote(install_dir)}
uv venv .venv
uv pip install -e .
"""
    _ssh_cmd(studio_ssh, remote_install)

    # launchd plist (system-wide)
    registry_path = os.environ.get("MLX_OMNI_REGISTRY_PATH", DEFAULT_REGISTRY)
    hf_home = os.environ.get("HF_HOME", DEFAULT_HF_HOME)
    cache_max = os.environ.get("MLX_OMNI_CACHE_MAX_SIZE", "3")
    cache_ttl = os.environ.get("MLX_OMNI_CACHE_TTL_SECONDS", "0")
    prewarm = os.environ.get(
        "MLX_OMNI_PREWARM_MODELS",
        "txgsync-gpt-oss-120b-derestricted-mxfp4-mlx,mlx-qwen3-next-80b-mxfp4-a3b-instruct,mlx-gpt-oss-20b-mxfp4-q4",
    )

    plist_path = f"/Library/LaunchDaemons/{label}.plist"
    log_path = f"/tmp/mlx-omni-{port}.log"
    program = f"{install_dir}/.venv/bin/mlx-omni-server"

    plist = f"""<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
  <key>Label</key><string>{label}</string>
  <key>WorkingDirectory</key><string>{install_dir}</string>
  <key>ProgramArguments</key>
  <array>
    <string>{program}</string>
    <string>--host</string><string>{bind_host}</string>
    <string>--port</string><string>{port}</string>
    <string>--workers</string><string>1</string>
    <string>--log-level</string><string>info</string>
  </array>
  <key>RunAtLoad</key><true/>
  <key>KeepAlive</key><true/>
  <key>StandardOutPath</key><string>{log_path}</string>
  <key>StandardErrorPath</key><string>{log_path}</string>
  <key>EnvironmentVariables</key>
  <dict>
    <key>HF_HOME</key><string>{hf_home}</string>
    <key>MLX_OMNI_REGISTRY_PATH</key><string>{registry_path}</string>
    <key>MLX_OMNI_CACHE_MAX_SIZE</key><string>{cache_max}</string>
    <key>MLX_OMNI_CACHE_TTL_SECONDS</key><string>{cache_ttl}</string>
    <key>MLX_OMNI_PREWARM_MODELS</key><string>{prewarm}</string>
  </dict>
</dict>
</plist>
"""

    remote_plist = f"""
set -euo pipefail
cat > {shlex.quote(plist_path)} <<'PLIST'
{plist}
PLIST
chown root:wheel {shlex.quote(plist_path)}
chmod 0644 {shlex.quote(plist_path)}
# bootout-by-path is more reliable than bootout-by-label on recent macOS.
launchctl bootout system {shlex.quote(plist_path)} >/dev/null 2>&1 || true
launchctl bootstrap system {shlex.quote(plist_path)}
launchctl kickstart -k system/{shlex.quote(label)}
"""
    _studio_sudo_cmd(studio_ssh, remote_plist)

    # Basic health
    _ssh_cmd(studio_ssh, f"curl -fsS http://127.0.0.1:{port}/v1/models | head -c 200 >/dev/null")
    print(f"omni installed and started: {label} (port {port})")


def cmd_omni_stop(args):
    studio_ssh = os.environ.get("STUDIO_SSH", "studio")
    port = int(args.port)
    label_prefix = os.environ.get("MLX_OMNI_LABEL_PREFIX", DEFAULT_OMNI_LABEL_PREFIX)
    label = f"{label_prefix}.{port}"
    plist_path = f"/Library/LaunchDaemons/{label}.plist"
    remote = f"""
set -euo pipefail
launchctl bootout system {shlex.quote(plist_path)} >/dev/null 2>&1 || true
rm -f {shlex.quote(plist_path)}
"""
    _studio_sudo_cmd(studio_ssh, remote, check=False)
    print(f"omni stopped: {label}")


def cmd_omni_status(args):
    studio_ssh = os.environ.get("STUDIO_SSH", "studio")
    port = int(args.port)
    _studio_sudo_cmd(studio_ssh, f"lsof -nP -iTCP:{port} -sTCP:LISTEN || true", check=False)
    _ssh_cmd(
        studio_ssh,
        f"curl -fsS http://127.0.0.1:{port}/v1/models | jq -r '.data[].id' 2>/dev/null || curl -fsS http://127.0.0.1:{port}/v1/models",
        check=False,
    )


def cmd_omni_warm(args):
    """Warm one or more models into the running mlx-omni-server process.

    Omni loads models lazily on first request; warming makes the first "real" request fast.
    """
    studio_ssh = os.environ.get("STUDIO_SSH", "studio")
    port = int(args.port)
    timeout = int(args.timeout)
    max_tokens = int(args.max_tokens)

    for model in args.models:
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": "warm"}],
            "max_tokens": max_tokens,
            "temperature": 0,
        }
        script = f"""
import json
from urllib import request as urlrequest

payload = {json.dumps(payload)}
req = urlrequest.Request(
    "http://127.0.0.1:{port}/v1/chat/completions",
    data=json.dumps(payload).encode("utf-8"),
    headers={{"Content-Type": "application/json"}},
    method="POST",
)
with urlrequest.urlopen(req, timeout={timeout}) as resp:
    resp.read()
print("warmed", payload.get("model"))
"""
        _ssh_cmd(studio_ssh, "python3 -", input_bytes=script.encode("utf-8"), check=True)


def cmd_mlx_launch_stop(args):
    """Stop the legacy mlx-openai-server supervisor launchd job (non-interactive).

    This does NOT mutate registry port assignments; it only stops the running services so
    we can free ports for mlx-omni-server cutover.
    """
    studio_ssh = os.environ.get("STUDIO_SSH", "studio")
    label = os.environ.get("MLX_OPENAI_LAUNCHD_LABEL", DEFAULT_MLX_OPENAI_LAUNCHD_LABEL)
    ports = (args.ports or "8100,8101,8102,8103").strip()

    # Stop the supervisor first (prevents immediate respawn).
    _studio_sudo_cmd(
        studio_ssh,
        f"launchctl bootout system/{shlex.quote(label)} >/dev/null 2>&1 || true",
        check=False,
    )
    # Prevent it from re-loading on reboot (can be re-enabled via mlx-launch-start).
    _studio_sudo_cmd(
        studio_ssh,
        f"launchctl disable system/{shlex.quote(label)} >/dev/null 2>&1 || true",
        check=False,
    )

    # Ensure legacy listeners are gone (they can outlive the supervisor).
    # Be conservative: only kill processes that look like mlx-openai-server, so we don't
    # accidentally terminate mlx-omni-server if ports overlap.
    remote = ["set -euo pipefail"]
    for raw in ports.split(","):
        raw = raw.strip()
        if not raw:
            continue
        port = int(raw)
        remote.append(f"pids=$(lsof -tiTCP:{port} -sTCP:LISTEN 2>/dev/null || true)")
        remote.append('for pid in ${pids}; do')
        remote.append('  cmd=$(ps -p ${pid} -o command= 2>/dev/null || true)')
        remote.append('  echo \"${cmd}\" | rg -q \"mlx-openai-server|mlx_openai_server\" || continue')
        remote.append('  kill ${pid} >/dev/null 2>&1 || true; sleep 0.2; kill -9 ${pid} >/dev/null 2>&1 || true')
        remote.append('done')
    _studio_sudo_cmd(studio_ssh, "\n".join(remote), check=False)
    print(f"stopped legacy supervisor {label} and cleared listeners for ports: {ports}")


def cmd_mlx_launch_start(args):
    """Start the legacy mlx-openai-server supervisor launchd job (rollback helper)."""
    studio_ssh = os.environ.get("STUDIO_SSH", "studio")
    label = os.environ.get("MLX_OPENAI_LAUNCHD_LABEL", DEFAULT_MLX_OPENAI_LAUNCHD_LABEL)
    plist = os.environ.get("MLX_OPENAI_LAUNCHD_PLIST", f"/Library/LaunchDaemons/{label}.plist")
    remote = f"""
set -euo pipefail
if [ ! -f {shlex.quote(plist)} ]; then
  echo "missing launchd plist: {plist}" >&2
  exit 2
fi
launchctl enable system/{shlex.quote(label)} >/dev/null 2>&1 || true
launchctl bootout system/{shlex.quote(label)} >/dev/null 2>&1 || true
launchctl bootstrap system {shlex.quote(plist)}
launchctl kickstart -k system/{shlex.quote(label)}
"""
    _studio_sudo_cmd(studio_ssh, remote, check=True)
    print(f"started legacy supervisor {label}")

def _collect_offload_candidates(studio_ssh: str):
    script = """
import json
from pathlib import Path

hub = Path("/Users/thestudio/models/hf/hub")
reg = hub / "registry.json"
if not reg.exists():
    raise SystemExit(0)
data = json.loads(reg.read_text() or "{}")
models = data.get("models", {})
for entry in models.values():
    og_path = entry.get("og_path")
    cache_path = entry.get("cache_path")
    if not og_path or not cache_path:
        continue
    if og_path == cache_path:
        continue
    try:
        og_path = Path(og_path)
        rel = og_path.relative_to(hub)
    except Exception:
        continue
    print(str(rel))
"""
    result = subprocess.run(
        ["ssh", studio_ssh, "python3", "-"],
        input=script.encode("utf-8"),
        capture_output=True,
        check=False,
    )
    if result.returncode != 0:
        raise SystemExit("failed to build offload list from studio registry")
    lines = [line.strip() for line in result.stdout.decode("utf-8").splitlines() if line.strip()]
    return lines


def cmd_offload_og(args):
    studio_ssh = os.environ.get("STUDIO_SSH", "thestudio@192.168.1.72")
    offload_script = Path(os.environ.get("MLX_OFFLOAD_SCRIPT", DEFAULT_OFFLOAD_SCRIPT))

    candidates = _collect_offload_candidates(studio_ssh)
    if not candidates:
        print("no offload candidates found")
        return
    if args.dry_run:
        print("\n".join(candidates))
        return

    with tempfile.NamedTemporaryFile("w", delete=False) as handle:
        for line in candidates:
            handle.write(line + "\n")
        list_path = handle.name

    if offload_script.exists():
        env = os.environ.copy()
        env["MLX_OFFLOAD_ALLOW"] = "1"
        env["MLX_OFFLOAD_LIST"] = list_path
        env.setdefault("STUDIO_SSH", studio_ssh)
        subprocess.run([str(offload_script)], env=env, check=False)
        return

    offload_host = os.environ.get("MLX_OFFLOAD_HOST", "mini")
    if not offload_host:
        raise SystemExit(f"offload script not found locally and MLX_OFFLOAD_HOST not set: {offload_script}")

    remote_list = f"/tmp/mlx_offload_{os.getpid()}.txt"
    subprocess.run(
        ["ssh", offload_host, f"cat > {remote_list}"],
        input=Path(list_path).read_bytes(),
        check=False,
    )
    env_parts = [
        "MLX_OFFLOAD_ALLOW=1",
        f"MLX_OFFLOAD_LIST={remote_list}",
        f"STUDIO_SSH={studio_ssh}",
    ]
    subprocess.run(
        ["ssh", offload_host, " ".join(env_parts + [str(offload_script)])],
        check=False,
    )
    subprocess.run(["ssh", offload_host, "rm", "-f", remote_list], check=False)


def main():
    parser = argparse.ArgumentParser(
        prog="mlxctl",
        description=(
            "MLX model registry + port controller for the Studio. "
            "Manages HF cache, registry.json, MLX servers, and LiteLLM sync."
        ),
        epilog=(
            "Naming (default): model_id becomes 'mlx-<family>-<params>-<quant>-<variant>'.\n"
            "Order matters: family  parameter count  quant  other identifiers (e.g., instruct).\n"
            "Repo org is stripped; dash-only; use --name only for exceptions.\n\n"
            "Common commands:\n"
            "  mlxctl list\n"
            "  mlxctl status\n"
            "  mlxctl load <repo_id> 8101\n"
            "  mlxctl unload 8101\n"
            "  mlxctl sync-gateway\n\n"
            "Ports:\n"
            "  8100-8119 team, 8120-8139 experimental\n"
        ),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("--local", action="store_true", help="run locally on Studio")
    sub = parser.add_subparsers(dest="command", required=True)

    sub.add_parser("init", help="Scan HF cache and initialize registry entries")
    sub.add_parser("list", help="List registered MLX models (model_id, repo_id, port, ctx, max_out)")
    sub.add_parser("status", help="Show MLX port status and routing mode")
    omni_install = sub.add_parser("omni-install", help="Install + run mlx-omni-server on the Studio (canary)")
    omni_install.add_argument("--port", default="8120", help="Port to run omni on (default: 8120)")
    omni_install.add_argument("--host", default="0.0.0.0", help="Bind address (default: 0.0.0.0)")
    omni_stop = sub.add_parser("omni-stop", help="Stop mlx-omni-server launchd job on the Studio")
    omni_stop.add_argument("--port", default="8120", help="Port label to stop (default: 8120)")
    omni_status = sub.add_parser("omni-status", help="Check mlx-omni-server status on the Studio")
    omni_status.add_argument("--port", default="8120", help="Port to check (default: 8120)")
    omni_warm = sub.add_parser("omni-warm", help="Warm models into mlx-omni-server cache (Omni-first)")
    omni_warm.add_argument("models", nargs="+", help="Model IDs to warm (e.g. openai/mlx-..., mlx-..., etc.)")
    omni_warm.add_argument("--port", default="8100", help="Omni port to warm against (default: 8100)")
    omni_warm.add_argument("--timeout", default="60", help="Per-model request timeout seconds (default: 60)")
    omni_warm.add_argument("--max-tokens", dest="max_tokens", default="8", help="Warmup max_tokens (default: 8)")
    mlx_launch_stop = sub.add_parser("mlx-launch-stop", help="Stop legacy com.bebop.mlx-launch (non-interactive)")
    mlx_launch_stop.add_argument("--ports", default="8100,8101,8102,8103", help="Comma-separated ports to clear listeners")
    sub.add_parser("mlx-launch-start", help="Start legacy com.bebop.mlx-launch (rollback helper)")

    load = sub.add_parser("load", help="Download/register a model and launch on a port")
    load.add_argument("model", help="HF repo_id or existing registry model_id")
    load.add_argument("port", help="Port number (8100-8139) or 'auto' for 8120+")
    load.add_argument("--force", action="store_true", help="Force reload even if port/model is already assigned")
    load.add_argument("--ignore-launchd", action="store_true", help="Skip launchd stop prompt")
    load.add_argument("--convert", choices=["auto", "force", "skip"], default="auto")
    load.add_argument("--name", help="Canonical model id override (defaults to mlx-<repo-name>, no org prefix)")
    load.add_argument("--no-offload-og", dest="offload_og", action="store_false", help="Skip offloading OG weights after conversion")
    load.set_defaults(offload_og=True)
    load.add_argument(
        "--preconverted",
        choices=["yes", "no", "auto"],
        default="auto",
        help="Override pre-converted detection (yes|no|auto)",
    )
    load.add_argument("--no-sync", dest="sync", action="store_false", help="Skip gateway sync after load")
    load.set_defaults(sync=True)

    unload = sub.add_parser("unload", help="Stop server on a port and clear registry assignment")
    unload.add_argument("port", help="Port number (8100-8139)")
    unload.add_argument("--no-sync", dest="sync", action="store_false", help="Skip gateway sync after unload")
    unload.set_defaults(sync=True)

    unload_all = sub.add_parser("unload-all", help="Stop all MLX servers and clear assignments")
    unload_all.add_argument("--no-sync", dest="sync", action="store_false", help="Skip gateway sync after unload-all")
    unload_all.set_defaults(sync=True)
    sub.add_parser("reconcile", help="Clear registry entries for ports no longer listening")
    sub.add_parser("verify", help="Validate registry vs gateway config")
    # Deprecated: ensemble (kept for backward compatibility)
    ensemble = sub.add_parser("ensemble", help="DEPRECATED: use load/assign-team instead")
    ensemble.add_argument("name", help="Deprecated")
    ensemble.add_argument("--force", action="store_true")
    ensemble.add_argument("--ignore-launchd", action="store_true")
    ensure = sub.add_parser("ensure", help="Ensure model is in registry (optionally load)")
    ensure.add_argument("repo", help="HF repo_id")
    ensure.add_argument("--name", help="Canonical model id override (defaults to mlx-<repo-name>, no org prefix)")
    ensure.add_argument("--convert", choices=["auto", "force", "skip"], default="auto")
    ensure.add_argument("--no-offload-og", dest="offload_og", action="store_false")
    ensure.set_defaults(offload_og=True)
    ensure.add_argument(
        "--preconverted",
        choices=["yes", "no", "auto"],
        default="auto",
        help="Override pre-converted detection (yes|no|auto)",
    )
    ensure.add_argument("--load", action="store_true", help="Load the model after ensuring it")
    ensure.add_argument("--port", default="auto", help="Port number (8100-8139) or 'auto' for 8120+")
    ensure.add_argument("--no-sync", dest="sync", action="store_false")
    ensure.set_defaults(sync=True)
    offload = sub.add_parser("offload-og", help="Offload OG base weights not used for inference")
    offload.add_argument("--dry-run", action="store_true")
    sync = sub.add_parser("sync-gateway", help="Sync MLX registry to LiteLLM router/env")
    sync.add_argument("--dry-run", action="store_true")
    assign = sub.add_parser("assign-team", help="Assign MLX models to team ports by size")
    assign.add_argument("--dry-run", action="store_true")
    assign.add_argument("--no-sync", dest="sync", action="store_false")
    assign.set_defaults(sync=True)

    args = parser.parse_args()
    _maybe_forward_to_studio(args)

    commands = {
        "init": cmd_init,
        "list": cmd_list,
        "status": cmd_status,
        "omni-install": cmd_omni_install,
        "omni-stop": cmd_omni_stop,
        "omni-status": cmd_omni_status,
        "omni-warm": cmd_omni_warm,
        "mlx-launch-stop": cmd_mlx_launch_stop,
        "mlx-launch-start": cmd_mlx_launch_start,
        "load": cmd_load,
        "unload": cmd_unload,
        "unload-all": cmd_unload_all,
        "reconcile": cmd_reconcile,
        "verify": cmd_verify,
        "ensemble": cmd_ensemble,
        "ensure": lambda args: cmd_ensure(args),
        "offload-og": cmd_offload_og,
        "sync-gateway": cmd_sync_gateway,
        "assign-team": cmd_assign_team,
    }
    commands[args.command](args)


if __name__ == "__main__":
    main()
