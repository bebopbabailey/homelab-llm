#!/usr/bin/env python3
import argparse
import json
import os
import re
import shlex
import socket
import subprocess
import sys
import time
import shutil
import tempfile
from contextlib import contextmanager
from pathlib import Path
from typing import Optional

try:
    import fcntl
except ImportError:  # pragma: no cover - non-POSIX platforms
    fcntl = None


DEFAULT_REGISTRY = "/Users/thestudio/models/hf/hub/registry.json"
DEFAULT_HF_HOME = "/Users/thestudio/models/hf"
DEFAULT_OFFLOAD_SCRIPT = "/home/christopherbailey/homelab-llm/logs/mlx_offload.sh"
DEFAULT_CONVERT_CMD = "python3 -m mlx_lm.convert --hf-path {src} --mlx-path {dest}"
DEFAULT_GATEWAY_ROOT = "/home/christopherbailey/homelab-llm"
DEFAULT_GATEWAY_HOST = "mini"
DEFAULT_GATEWAY_MLXCTL = "/home/christopherbailey/homelab-llm/platform/ops/scripts/mlxctl"
DEFAULT_HANDLES_PATH = "/home/christopherbailey/homelab-llm/layer-gateway/registry/handles.jsonl"
DEFAULT_ENSEMBLES_DOC = "/home/christopherbailey/homelab-llm/layer-gateway/optillm-proxy/ENSEMBLES.md"
DEFAULT_MLX_HOST = "192.168.1.72"
DEFAULT_ROUTE_VIA_OPTILLM = True
DEFAULT_CONTEXT_LENGTH = 131072
DEFAULT_MAX_OUTPUT_TOKENS = 64000

TEAM_RANGE = range(8100, 8120)
EXPERIMENTAL_RANGE = range(8120, 8140)
PORT_RANGE = range(8100, 8140)

PORT_MAP = {}

ENSEMBLES = {}


def _is_local_host():
    host = socket.gethostname().split(".")[0].lower()
    return host in {"studio", "thestudio"}


def _maybe_forward_to_studio(args):
    if args.local:
        return
    if getattr(args, "command", None) == "offload-og":
        return
    if getattr(args, "command", None) == "sync-gateway":
        return
    if getattr(args, "command", None) == "assign-team":
        return
    if _is_local_host():
        return
    studio_host = os.environ.get("STUDIO_HOST", "studio")
    studio_cmd = os.environ.get("STUDIO_MLXCTL", "/Users/thestudio/bin/mlxctl")
    cmd = ["ssh", studio_host, studio_cmd, "--local"] + sys.argv[1:]
    raise SystemExit(subprocess.call(cmd))


def _registry_path():
    return Path(os.environ.get("MLX_REGISTRY_PATH", DEFAULT_REGISTRY))


def _hf_home():
    return Path(os.environ.get("HF_HOME", DEFAULT_HF_HOME))


@contextmanager
def _registry_lock(path: Path):
    lock_path = path.with_suffix(".lock")
    lock_path.parent.mkdir(parents=True, exist_ok=True)
    with lock_path.open("w") as lock_handle:
        if fcntl:
            fcntl.flock(lock_handle.fileno(), fcntl.LOCK_EX)
        yield
        if fcntl:
            fcntl.flock(lock_handle.fileno(), fcntl.LOCK_UN)


def _load_registry(path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    if not path.exists():
        path.write_text(json.dumps({"version": 1, "models": {}}, indent=2))
    with _registry_lock(path):
        data = json.loads(path.read_text() or "{}")
    data.setdefault("version", 1)
    data.setdefault("models", {})
    return data


def _port_for_model(model_id: str, data):
    entry = data.get("models", {}).get(model_id)
    if entry and entry.get("port"):
        return int(entry["port"])
    for slug, row in data.get("models", {}).items():
        if row.get("model_id") == model_id and row.get("port"):
            return int(row["port"])
    return PORT_MAP.get(model_id)


def _load_ensembles_doc(path: Path):
    if not path.exists():
        return {}
    handle = None
    ensembles = {}
    for raw in path.read_text().splitlines():
        line = raw.strip()
        match = re.match(r"- \*\*(opt-[^*]+)\*\*", line)
        if match:
            handle = match.group(1)
            ensembles.setdefault(handle, [])
            continue
        if not handle:
            continue
        if line.startswith("- Base:") or line.startswith("- Bases:"):
            models = re.findall(r"`([^`]+)`", line)
            if models:
                ensembles[handle].extend(models)
    return ensembles


def _write_registry(path: Path, data):
    tmp = path.with_suffix(".tmp")
    payload = json.dumps(data, indent=2)
    with _registry_lock(path):
        tmp.write_text(payload)
        tmp.replace(path)


def _slugify(text: str):
    text = text.lower().replace("/", "-")
    text = re.sub(r"[^a-z0-9-_]+", "-", text).strip("-")
    return text


def _repo_from_dir(name: str):
    if not name.startswith("models--"):
        return None
    parts = name.replace("models--", "", 1).split("--", 1)
    if len(parts) != 2:
        return None
    return f"{parts[0]}/{parts[1]}"


def _latest_snapshot(model_dir: Path):
    snapshots = model_dir / "snapshots"
    if not snapshots.exists():
        return None
    candidates = list(snapshots.iterdir())
    if not candidates:
        return None
    return max(candidates, key=lambda p: p.stat().st_mtime)


def _guess_format(repo_id: str):
    return "gguf" if "gguf" in repo_id.lower() else "mlx"


def _find_entry(data, model_ref: str):
    models = data.get("models", {})
    if model_ref in models:
        return model_ref, models[model_ref]
    for slug, entry in models.items():
        if entry.get("repo_id") == model_ref:
            return slug, entry
    return None, None


def _is_preconverted(repo_id: str, override: Optional[str] = None):
    if override == "yes":
        return True
    if override == "no":
        return False
    org, _sep, _name = repo_id.partition("/")
    orgs = os.environ.get("MLX_PRECONVERTED_ORGS", "mlx-community,mlx,ml-explore").split(",")
    orgs = {o.strip().lower() for o in orgs if o.strip()}
    hints = os.environ.get("MLX_PRECONVERTED_HINTS", "mlx").split(",")
    hints = {h.strip().lower() for h in hints if h.strip()}
    repo_lower = repo_id.lower()
    return org.lower() in orgs or any(hint in repo_lower for hint in hints)


def _port_listening(port: int):
    try:
        result = subprocess.run(
            ["lsof", "-tiTCP:%d" % port, "-sTCP:LISTEN"],
            capture_output=True,
            text=True,
            check=False,
        )
    except FileNotFoundError:
        raise SystemExit("lsof is required on the Studio to detect ports.")
    pids = [line for line in result.stdout.splitlines() if line.strip()]
    return pids


def _ps_snapshot():
    try:
        result = subprocess.run(
            ["ps", "-ax", "-o", "pid=,ppid=,command="],
            capture_output=True,
            text=True,
            check=False,
        )
    except FileNotFoundError:
        raise SystemExit("ps is required on the Studio to inspect processes.")
    rows = []
    for line in result.stdout.splitlines():
        line = line.strip()
        if not line:
            continue
        parts = line.split(None, 2)
        if len(parts) < 3:
            continue
        pid, ppid, cmd = parts
        rows.append((int(pid), int(ppid), cmd))
    return rows


def _mlx_pids_for_port(port: int):
    port_token = re.compile(rf"--port(?:=|\s+){port}\b")
    pids = []
    for pid, _ppid, cmd in _ps_snapshot():
        if "mlx-openai-server" not in cmd:
            continue
        if port_token.search(cmd):
            pids.append(pid)
    return pids


def _include_children(pids):
    snapshot = _ps_snapshot()
    by_ppid = {}
    for pid, ppid, cmd in snapshot:
        by_ppid.setdefault(ppid, []).append((pid, cmd))
    expanded = set(pids)
    queue = list(pids)
    while queue:
        current = queue.pop()
        for child_pid, _cmd in by_ppid.get(current, []):
            if child_pid not in expanded:
                expanded.add(child_pid)
                queue.append(child_pid)
    return sorted(expanded)


def _kill_pids(pids, timeout: int = 10):
    if not pids:
        return
    for pid in pids:
        subprocess.run(["kill", str(pid)], check=False)
    deadline = time.time() + timeout
    remaining = set(pids)
    while remaining and time.time() < deadline:
        live = set(pid for pid, _ppid, _cmd in _ps_snapshot())
        remaining = remaining.intersection(live)
        if remaining:
            time.sleep(0.5)
    if remaining:
        for pid in remaining:
            subprocess.run(["kill", "-9", str(pid)], check=False)


def _stop_mlx_for_port(port: int):
    pids = _mlx_pids_for_port(port)
    if not pids:
        return
    tree = _include_children(pids)
    _kill_pids(tree)


def _launchd_loaded(label: str):
    if sys.platform != "darwin":
        return False
    result = subprocess.run(
        ["launchctl", "print", f"system/{label}"],
        capture_output=True,
        text=True,
        check=False,
    )
    return result.returncode == 0


def _maybe_stop_launchd(label: str):
    if not _launchd_loaded(label):
        return
    if os.geteuid() != 0:
        print(f"launchd job {label} is loaded; stop it with:")
        print(f"sudo launchctl bootout system/{label}")
        return
    subprocess.run(["launchctl", "bootout", f"system/{label}"], check=False)


def _prompt_yes_no(message: str):
    reply = input(f"{message} [y/N]: ").strip().lower()
    return reply in {"y", "yes"}


def _model_on_port(data, port: int):
    for slug, entry in data["models"].items():
        if entry.get("port") == port:
            return slug, entry
    return None, None


def _wait_for_port(port: int, timeout: int = 60):
    start = time.time()
    while time.time() - start < timeout:
        if _port_listening(port):
            return True
        time.sleep(1)
    return False


def _available_port(data, preferred_range):
    assigned = {entry.get("port") for entry in data.get("models", {}).values() if entry.get("port")}
    for port in preferred_range:
        if port in assigned:
            continue
        if _port_listening(port) or _mlx_pids_for_port(port):
            continue
        return port
    return None


def cmd_init(_args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    models = data["models"]

    hub = _hf_home() / "hub"
    if not hub.exists():
        raise SystemExit(f"HF hub directory not found: {hub}")

    added = 0
    for model_dir in hub.iterdir():
        repo_id = _repo_from_dir(model_dir.name)
        if not repo_id:
            continue
        slug = _slugify(repo_id)
        if slug in models:
            continue
        snapshot = _latest_snapshot(model_dir)
        if not snapshot:
            continue
        models[slug] = {
            "repo_id": repo_id,
            "model_id": slug,
            "cache_path": str(snapshot),
            "format": _guess_format(repo_id),
            "port": None,
        }
        added += 1

    _write_registry(registry_path, data)
    print(f"registry initialized: {registry_path} (added {added})")


def cmd_list(_args):
    data = _load_registry(_registry_path())
    for slug, entry in sorted(data["models"].items()):
        port = entry.get("port")
        repo_id = entry.get("repo_id")
        ctx = entry.get("context_length") or "-"
        max_out = entry.get("max_output_tokens") or "-"
        print(f"{slug}\t{repo_id}\t{port if port else '-'}\t{ctx}\t{max_out}")


def cmd_status(_args):
    data = _load_registry(_registry_path())
    env_flag = os.environ.get("MLX_ROUTE_VIA_OPTILLM")
    route_via_optillm = DEFAULT_ROUTE_VIA_OPTILLM
    if env_flag is not None:
        route_via_optillm = env_flag.strip().lower() not in {"0", "false", "no"}
    print(f"route_via_optillm\t{'on' if route_via_optillm else 'off'}")
    if route_via_optillm:
        gateway_host = os.environ.get("GATEWAY_HOST", DEFAULT_GATEWAY_HOST)
        cmd = ["ssh", gateway_host, "curl -sS -m 3 http://127.0.0.1:4020/v1/models >/dev/null && echo ok || echo down"]
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)
            status = result.stdout.strip() or "unknown"
        except Exception:
            status = "unknown"
        print(f"optillm_proxy\t{status}")
    port_map = {entry.get("port"): slug for slug, entry in data["models"].items() if entry.get("port")}
    for port in PORT_RANGE:
        pids = _port_listening(port)
        proc_pids = _mlx_pids_for_port(port)
        slug = port_map.get(port)
        status = "listening" if pids else ("starting" if proc_pids else "idle")
        pid_str = ",".join(pids if pids else [str(pid) for pid in proc_pids]) if (pids or proc_pids) else "-"
        model_str = slug if slug else "-"
        print(f"{port}\t{status}\t{pid_str}\t{model_str}")


def _infer_context_length(entry: dict) -> Optional[int]:
    cache_path = entry.get("cache_path")
    if not cache_path:
        return None
    path = Path(cache_path)
    candidates = [path / "config.json", path.parent / "config.json"]
    for cfg in candidates:
        if not cfg.exists():
            continue
        try:
            data = json.loads(cfg.read_text())
        except Exception:
            continue
        for key in ("max_position_embeddings", "model_max_length", "n_positions", "max_sequence_length"):
            val = data.get(key)
            if isinstance(val, int) and val > 0 and val < 1_000_000:
                return val
    return None


def _ensure_defaults(entry: dict):
    if not entry.get("context_length"):
        inferred = _infer_context_length(entry)
        entry["context_length"] = inferred or DEFAULT_CONTEXT_LENGTH
    if not entry.get("max_output_tokens"):
        entry["max_output_tokens"] = min(DEFAULT_MAX_OUTPUT_TOKENS, int(entry["context_length"]))


def _download_and_register(data, model_ref: str):
    try:
        from huggingface_hub import snapshot_download
    except ImportError as exc:
        raise SystemExit("huggingface_hub is required for downloads. Install it first.") from exc

    cache_dir = _hf_home()
    repo_id = model_ref
    snapshot = snapshot_download(repo_id, cache_dir=str(cache_dir))
    slug = _slugify(repo_id)
    data["models"][slug] = {
        "repo_id": repo_id,
        "model_id": slug,
        "cache_path": str(snapshot),
        "format": _guess_format(repo_id),
        "port": None,
    }
    return slug, data["models"][slug]


def _run_convert(repo_id: str, src_path: Path, dest_path: Path):
    cmd_tmpl = os.environ.get("MLX_CONVERT_CMD", DEFAULT_CONVERT_CMD)
    cmd = cmd_tmpl.format(repo=repo_id, src=src_path, dest=dest_path)
    argv = shlex.split(cmd)
    if not argv:
        raise SystemExit("MLX_CONVERT_CMD is empty")
    dest_path.mkdir(parents=True, exist_ok=True)
    result = subprocess.run(argv, check=False)
    if result.returncode != 0:
        raise SystemExit(f"conversion failed (exit {result.returncode})")


def _ensure_model(data, model_ref: str, name: Optional[str], convert_mode: str, preconverted: Optional[str]):
    slug, entry = _find_entry(data, model_ref)
    if entry:
        return slug, entry, False

    try:
        from huggingface_hub import snapshot_download
    except ImportError as exc:
        raise SystemExit("huggingface_hub is required for downloads. Install it first.") from exc

    cache_dir = _hf_home()
    repo_id = model_ref
    snapshot = snapshot_download(repo_id, cache_dir=str(cache_dir))
    slug = name or _slugify(repo_id)

    preconverted = _is_preconverted(repo_id, preconverted)
    do_convert = convert_mode == "force" or (convert_mode == "auto" and not preconverted)
    if convert_mode == "skip":
        do_convert = False

    repo_root = Path(snapshot).parent.parent
    entry = {
        "repo_id": repo_id,
        "model_id": slug,
        "cache_path": str(snapshot),
        "format": _guess_format(repo_id),
        "port": None,
    }

    if do_convert:
        hub = _hf_home() / "hub"
        dest = hub / f"converted--{slug}"
        if not dest.exists() or not any(dest.iterdir()):
            _run_convert(repo_id, Path(snapshot), dest)
        entry["og_path"] = str(repo_root)
        entry["cache_path"] = str(dest)
        entry["format"] = "mlx"

    data["models"][slug] = entry
    return slug, entry, do_convert


def _launch_server(entry, port: int):
    model_path = entry["cache_path"]
    cmd = os.environ.get("MLX_LAUNCH_CMD")
    if cmd:
        argv = cmd.split()
    elif shutil.which("mlx-openai-server"):
        argv = ["mlx-openai-server", "launch"]
    elif shutil.which("uv"):
        argv = ["uv", "run", "--project", "/opt/mlx-launch", "mlx-openai-server", "launch"]
    elif Path("/opt/homebrew/bin/uv").exists():
        argv = ["/opt/homebrew/bin/uv", "run", "--project", "/opt/mlx-launch", "mlx-openai-server", "launch"]
    else:
        raise SystemExit("mlx-openai-server not found; set MLX_LAUNCH_CMD or install it.")

    argv += [
        "--model-path",
        model_path,
        "--model-type",
        "lm",
        "--trust-remote-code",
        "--host",
        "0.0.0.0",
        "--port",
        str(port),
    ]

    tool_call_parser = entry.get("tool_call_parser") or os.environ.get("MLX_TOOL_CALL_PARSER")
    if tool_call_parser:
        argv += ["--tool-call-parser", tool_call_parser]

    reasoning_parser = entry.get("reasoning_parser") or os.environ.get("MLX_REASONING_PARSER")
    if reasoning_parser:
        argv += ["--reasoning-parser", reasoning_parser]

    chat_template = entry.get("chat_template") or os.environ.get("MLX_CHAT_TEMPLATE")
    if chat_template:
        argv += ["--chat-template-file", chat_template]

    log_dir = os.environ.get("MLX_LOG_DIR", "/tmp")
    Path(log_dir).mkdir(parents=True, exist_ok=True)
    log_file = Path(log_dir) / f"mlx-{port}.log"
    with log_file.open("a") as handle:
        subprocess.Popen(argv, stdout=handle, stderr=handle)


def cmd_load(args):
    if args.port == "auto":
        registry_path = _registry_path()
        data = _load_registry(registry_path)
        port = _available_port(data, EXPERIMENTAL_RANGE)
        if port is None:
            raise SystemExit("no free ports available in experimental range (8120-8139)")
        args.port = str(port)
    port = int(args.port)
    if port not in PORT_RANGE:
        raise SystemExit("port must be between 8100 and 8139")

    registry_path = _registry_path()
    data = _load_registry(registry_path)
    launchd_label = os.environ.get("MLX_LAUNCHD_LABEL", "com.bebop.mlx-launch")
    if not args.ignore_launchd and _launchd_loaded(launchd_label):
        if _prompt_yes_no(f"launchd job {launchd_label} is loaded. Stop it before loading?"):
            _maybe_stop_launchd(launchd_label)
        else:
            raise SystemExit("load cancelled")

    running_pids = _mlx_pids_for_port(port)
    if (running_pids or _port_listening(port)) and not args.force:
        slug, _entry = _model_on_port(data, port)
        model_name = slug if slug else "unknown"
        if not _prompt_yes_no(f"model {model_name} already in port {port}. Unload model?"):
            raise SystemExit("load cancelled")
        cmd_unload(argparse.Namespace(port=str(port)))

    slug, entry = _find_entry(data, args.model)
    converted = False
    if not entry:
        slug, entry, converted = _ensure_model(data, args.model, args.name, args.convert, args.preconverted)
    _ensure_defaults(entry)

    if entry.get("port") and entry.get("port") != port:
        raise SystemExit(f"model already assigned to port {entry['port']}; unload first")

    _launch_server(entry, port)
    if not _wait_for_port(port):
        raise SystemExit(f"model failed to start on port {port}")

    entry["port"] = port
    _write_registry(registry_path, data)
    if converted and args.offload_og:
        cmd_offload_og(argparse.Namespace(dry_run=False))
    if args.sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False, route_via_optillm=None))
    print(f"loaded {slug} on {port}")


def cmd_unload(args):
    port = int(args.port)
    if port not in PORT_RANGE:
        raise SystemExit("port must be between 8100 and 8139")

    _stop_mlx_for_port(port)

    registry_path = _registry_path()
    data = _load_registry(registry_path)
    for entry in data["models"].values():
        if entry.get("port") == port:
            entry["port"] = None
    _write_registry(registry_path, data)
    if args.sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False, route_via_optillm=None))
    print(f"unloaded port {port}")


def cmd_unload_all(args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    running = []
    for port in PORT_RANGE:
        if _port_listening(port) or _mlx_pids_for_port(port):
            slug, _entry = _model_on_port(data, port)
            model_name = slug if slug else "unknown"
            running.append((port, model_name))
    if running:
        print("Currently running models:")
        for port, model_name in running:
            print(f"- {model_name} on {port}")
    else:
        print("No running models detected.")
        return
    if not _prompt_yes_no("Unload all models?"):
        print("unload-all cancelled")
        return
    for port in PORT_RANGE:
        _stop_mlx_for_port(port)
        for entry in data["models"].values():
            if entry.get("port") == port:
                entry["port"] = None
    _write_registry(registry_path, data)
    if args.sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False, route_via_optillm=None))
    print("unloaded all ports")


def cmd_reconcile(_args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    changed = 0
    for entry in data["models"].values():
        port = entry.get("port")
        if not port:
            continue
        if not _port_listening(int(port)):
            entry["port"] = None
            changed += 1
    _write_registry(registry_path, data)
    print(f"reconciled registry (cleared {changed} stale ports)")
    _verify_optillm_routing()


def cmd_verify(_args):
    data = _load_registry(_registry_path())
    missing = []
    for slug, entry in data["models"].items():
        if not entry.get("context_length"):
            missing.append(f"{slug}: missing context_length")
        if not entry.get("max_output_tokens"):
            missing.append(f"{slug}: missing max_output_tokens")
    if missing:
        raise SystemExit("mlxctl verify failed:\n  - " + "\n  - ".join(missing))
    _verify_optillm_routing()
    print("mlxctl verify ok: registry defaults and routing state are consistent")


def cmd_ensemble(args):
    raise SystemExit("mlxctl ensemble is deprecated; use load/assign-team instead")


def cmd_ensure(args):
    registry_path = _registry_path()
    data = _load_registry(registry_path)
    slug, entry, converted = _ensure_model(data, args.repo, args.name, args.convert, args.preconverted)
    _ensure_defaults(entry)
    _write_registry(registry_path, data)
    if converted and args.offload_og:
        cmd_offload_og(argparse.Namespace(dry_run=False))
    if args.load:
        cmd_load(
            argparse.Namespace(
                model=slug,
                port=args.port,
                force=False,
                ignore_launchd=False,
                convert="skip",
                name=None,
                offload_og=False,
                preconverted="auto",
                sync=args.sync,
            )
        )
    elif args.sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False, route_via_optillm=None))


def _fetch_registry_from_studio(studio_ssh: str, registry_path: str):
    script = f"""
import json
from pathlib import Path
path = Path({registry_path!r})
if not path.exists():
    raise SystemExit(1)
print(path.read_text())
"""
    result = subprocess.run(
        ["ssh", studio_ssh, "python3", "-"],
        input=script.encode("utf-8"),
        capture_output=True,
        check=False,
    )
    if result.returncode != 0:
        raise SystemExit("failed to read MLX registry from studio")
    return json.loads(result.stdout.decode("utf-8") or "{}")


def _fetch_registry_sizes(studio_ssh: str, registry_path: str):
    script = f"""
import json
import os
from pathlib import Path

path = Path({registry_path!r})
if not path.exists():
    raise SystemExit(1)
data = json.loads(path.read_text() or "{{}}")
models = data.get("models", {{}})

def repo_root(cache_path: str):
    p = Path(cache_path)
    if p.name.startswith("models--") or p.name.startswith("converted--"):
        return p
    for parent in p.parents:
        name = parent.name
        if name.startswith("models--") or name.startswith("converted--"):
            return parent
    return p

def dir_size(root: Path):
    total = 0
    for base, _dirs, files in os.walk(root, followlinks=False):
        for fname in files:
            fpath = os.path.join(base, fname)
            if os.path.islink(fpath):
                continue
            try:
                total += os.path.getsize(fpath)
            except OSError:
                continue
    return total

rows = []
for slug, entry in models.items():
    cache_path = entry.get("cache_path")
    if not cache_path:
        continue
    root = repo_root(cache_path)
    if not root.exists():
        continue
    rows.append({{
        "slug": slug,
        "model_id": entry.get("model_id") or slug,
        "cache_path": str(cache_path),
        "root": str(root),
        "size_bytes": dir_size(root),
        "port": entry.get("port"),
    }})
print(json.dumps(rows))
"""
    result = subprocess.run(
        ["ssh", studio_ssh, "python3", "-"],
        input=script.encode("utf-8"),
        capture_output=True,
        check=False,
    )
    if result.returncode != 0:
        raise SystemExit("failed to read MLX registry sizes from studio")
    return json.loads(result.stdout.decode("utf-8") or "[]")


def _optillm_models_from_handles(path: Path):
    if not path.exists():
        raise SystemExit(f"handles.jsonl not found: {path}")
    models = set()
    for line in path.read_text().splitlines():
        if not line.strip():
            continue
        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            continue
        handle = obj.get("handle", "")
        if not handle.startswith("opt-"):
            continue
        selector = obj.get("selector") or {}
        model = selector.get("model")
        if not model:
            continue
        for match in re.findall(r"mlx-[a-z0-9-]+", str(model)):
            models.add(match)
    return models


def _apply_port_map(assignments, studio_ssh: str, sync: bool):
    studio_cmd = os.environ.get("STUDIO_MLXCTL", "/Users/thestudio/bin/mlxctl")
    for handle, port in assignments:
        cmd = [
            "ssh",
            studio_ssh,
            studio_cmd,
            "--local",
            "load",
            handle,
            str(port),
            "--force",
            "--ignore-launchd",
            "--no-sync",
        ]
        subprocess.run(cmd, check=False)
    if sync:
        cmd_sync_gateway(argparse.Namespace(dry_run=False))


def cmd_assign_team(args):
    studio_ssh = os.environ.get("STUDIO_SSH", "thestudio@192.168.1.72")
    handles_path = Path(os.environ.get("HANDLES_PATH", DEFAULT_HANDLES_PATH))
    registry_path = os.environ.get("MLX_REGISTRY_PATH", DEFAULT_REGISTRY)

    # ensure registry includes all cached models
    subprocess.run(["ssh", studio_ssh, os.environ.get("STUDIO_MLXCTL", "/Users/thestudio/bin/mlxctl"), "--local", "init"], check=False)

    optillm_models = _optillm_models_from_handles(handles_path)
    rows = _fetch_registry_sizes(studio_ssh, registry_path)
    if not rows:
        raise SystemExit("no MLX models found in registry")

    by_handle = {row["model_id"]: row for row in rows}
    opt_rows = [row for handle, row in by_handle.items() if handle in optillm_models]
    other_rows = [row for handle, row in by_handle.items() if handle not in optillm_models]

    opt_rows.sort(key=lambda r: (-r["size_bytes"], r["model_id"]))
    other_rows.sort(key=lambda r: (-r["size_bytes"], r["model_id"]))

    opt_ports = list(range(8100, 8110))
    other_ports = list(range(8110, 8120))

    if len(opt_rows) > len(opt_ports):
        raise SystemExit("too many optillm models for 8100-8109")
    if len(other_rows) > len(other_ports):
        raise SystemExit("too many non-optillm models for 8110-8119")

    assignments = []
    for row, port in zip(opt_rows, opt_ports):
        assignments.append((row["model_id"], port))
    for row, port in zip(other_rows, other_ports):
        assignments.append((row["model_id"], port))

    if args.dry_run:
        print(json.dumps({"count": len(assignments), "assignments": assignments}, indent=2))
        return

    _apply_port_map(assignments, studio_ssh, sync=args.sync)


def _env_key(handle: str, suffix: str):
    key = handle.upper().replace("-", "_")
    return f"MLX_{key}_{suffix}"


def _build_mlx_entries(models, host: str):
    items = []
    for slug, entry in models.items():
        port = entry.get("port")
        if not port:
            continue
        handle = entry.get("model_id") or slug
        items.append(
            {
                "handle": handle,
                "port": int(port),
                "request_params": entry.get("request_params", {}),
                "context_length": entry.get("context_length"),
                "max_output_tokens": entry.get("max_output_tokens"),
            }
        )
    return sorted(items, key=lambda i: i["port"])



def _yaml_scalar(value):
    if value is None:
        return "null"
    if isinstance(value, bool):
        return "true" if value else "false"
    if isinstance(value, (int, float)):
        return str(value)
    if isinstance(value, str):
        return json.dumps(value)
    raise SystemExit(f"unsupported request_param type: {type(value)}")


def _update_router_yaml(path: Path, mlx_entries, route_via_optillm: bool):
    if not path.exists():
        raise SystemExit(f"router.yaml not found: {path}")
    lines = path.read_text().splitlines()
    begin = "# BEGIN MLXCTL AUTO"
    end = "# END MLXCTL AUTO"

    # Remove existing MLX entries (legacy or previous sync)
    new_lines = []
    skip = False
    skip_block = False
    for line in lines:
        if line.strip() == begin:
            skip = True
            continue
        if line.strip() == end:
            skip = False
            continue
        if skip:
            continue
        if line.startswith("- model_name: "):
            if line.startswith("- model_name: mlx-") or line.startswith("- model_name: router-mlx-"):
                skip_block = True
                continue
            skip_block = False
        if skip_block:
            continue
        new_lines.append(line)

    block = [begin]
    for item in mlx_entries:
        handle = item["handle"]
        env_base = _env_key(handle, "API_BASE")
        env_model = _env_key(handle, "MODEL")
        env_key = _env_key(handle, "API_KEY")
        ctx = item.get("context_length") or 65536
        max_output = item.get("max_output_tokens")
        if not max_output:
            max_output = min(32768, max(1024, int(ctx) // 2))
        block += [
            f"- model_name: {handle}",
            "  litellm_params:",
            f"    model: os.environ/{env_model}",
            f"    api_base: os.environ/{env_base}",
            f"    api_key: os.environ/{env_key}",
            f"    max_tokens: {max_output}",
        ]
        request_params = item.get("request_params") or {}
        if request_params:
            for key in sorted(request_params.keys()):
                block.append(f"    {key}: {_yaml_scalar(request_params[key])}")
        block += [
            "  model_info:",
            f"    max_input_tokens: {int(ctx)}",
            f"    max_output_tokens: {int(max_output)}",
        ]
        if route_via_optillm:
            router_handle = f"router-{handle}"
            router_env_base = _env_key(router_handle, "API_BASE")
            router_env_model = _env_key(router_handle, "MODEL")
            router_env_key = _env_key(router_handle, "API_KEY")
            block += [
                f"- model_name: {router_handle}",
                "  litellm_params:",
                f"    model: os.environ/{router_env_model}",
                f"    api_base: os.environ/{router_env_base}",
                f"    api_key: os.environ/{router_env_key}",
                f"    max_tokens: {max_output}",
                "  model_info:",
                f"    max_input_tokens: {int(ctx)}",
                f"    max_output_tokens: {int(max_output)}",
            ]
    block.append(end)

    inserted = False
    output = []
    for line in new_lines:
        output.append(line)
        if line.strip() == "model_list:" and not inserted:
            output.extend(block)
            inserted = True
    if not inserted:
        output = block + new_lines
    path.write_text("\n".join(output) + "\n")


def _update_env_local(path: Path, mlx_entries, host: str, route_via_optillm: bool):
    if not path.exists():
        raise SystemExit(f"env.local not found: {path}")
    begin = "# BEGIN MLXCTL AUTO"
    end = "# END MLXCTL AUTO"
    lines = path.read_text().splitlines()
    output = []
    skip = False
    for line in lines:
        if line.strip() == begin:
            skip = True
            continue
        if line.strip() == end:
            skip = False
            continue
        if skip:
            continue
        if line.startswith("JERRY_") or line.startswith("BENCH_") or line.startswith("MLX_") or line.startswith("ROUTER_"):
            continue
        output.append(line)

    block = [begin, "# MLX OpenAI server base URLs (auto-generated)"]
    for item in mlx_entries:
        handle = item["handle"]
        port = item["port"]
        if route_via_optillm:
            block.append(f"{_env_key(handle, 'API_BASE')}=http://127.0.0.1:4020/v1")
        else:
            block.append(f"{_env_key(handle, 'API_BASE')}=http://{host}:{port}/v1")
    block.append("")
    block.append("# Upstream model IDs (prefix with \"openai/\" for OpenAI-compatible backends)")
    for item in mlx_entries:
        handle = item["handle"]
        if route_via_optillm:
            block.append(f"{_env_key(handle, 'MODEL')}=openai/router-{handle}")
        else:
            block.append(f"{_env_key(handle, 'MODEL')}=openai/{handle}")
    block.append("")
    block.append("# API keys (set to \"dummy\" if the backend does not enforce auth)")
    for item in mlx_entries:
        handle = item["handle"]
        block.append(f"{_env_key(handle, 'API_KEY')}=dummy")
    if route_via_optillm:
        block.append("")
        block.append("# OptiLLM â†’ MLX (internal router targets)")
        for item in mlx_entries:
            handle = item["handle"]
            port = item["port"]
            router_handle = f"router-{handle}"
            block.append(f"{_env_key(router_handle, 'API_BASE')}=http://{host}:{port}/v1")
            block.append(f"{_env_key(router_handle, 'MODEL')}=openai/{handle}")
            block.append(f"{_env_key(router_handle, 'API_KEY')}=dummy")
    block.append(end)

    output.append("")
    output.extend(block)
    path.write_text("\n".join(output).rstrip() + "\n")


def _update_handles(path: Path, mlx_entries):
    if not path.exists():
        raise SystemExit(f"handles.jsonl not found: {path}")
    rows = []
    for line in path.read_text().splitlines():
        if not line.strip():
            continue
        try:
            obj = json.loads(line)
        except json.JSONDecodeError:
            rows.append(line)
            continue
        handle = obj.get("handle", "")
        endpoint = obj.get("endpoint_ref", "")
        if handle.startswith("mlx-") or endpoint.startswith("ep_mlx_slot_"):
            continue
        rows.append(obj)

    for item in mlx_entries:
        handle = item["handle"]
        port = item["port"]
        rows.append(
            {
                "handle": handle,
                "kind": "model",
                "invoke": "openai-chat",
                "managed_by": "launchd",
                "endpoint_ref": f"ep_mlx_slot_{port}",
                "selector": {"model": handle},
            }
        )

    with path.open("w") as handle_out:
        for row in rows:
            if isinstance(row, str):
                handle_out.write(row + "\n")
            else:
                handle_out.write(json.dumps(row, separators=(",", ":")) + "\n")


def _verify_optillm_routing():
    env_flag = os.environ.get("MLX_ROUTE_VIA_OPTILLM")
    route_via_optillm = DEFAULT_ROUTE_VIA_OPTILLM
    if env_flag is not None:
        route_via_optillm = env_flag.strip().lower() not in {"0", "false", "no"}
    if not route_via_optillm:
        return
    gateway_root = Path(os.environ.get("GATEWAY_ROOT", DEFAULT_GATEWAY_ROOT))
    router_path = gateway_root / "layer-gateway/litellm-orch/config/router.yaml"
    env_path = gateway_root / "layer-gateway/litellm-orch/config/env.local"
    if not router_path.exists() or not env_path.exists():
        raise SystemExit("mlxctl verify failed: gateway router/env not found")
    router_text = router_path.read_text()
    env_text = env_path.read_text()
    data = _load_registry(_registry_path())
    missing = []
    for slug, entry in data["models"].items():
        if not entry.get("port"):
            continue
        router_handle = f"router-{slug}"
        if f"- model_name: {router_handle}" not in router_text:
            missing.append(f"router.yaml missing {router_handle}")
        if f"{_env_key(router_handle, 'API_BASE')}=" not in env_text:
            missing.append(f"env.local missing {router_handle} API_BASE")
    if missing:
        raise SystemExit("mlxctl verify failed:\n  - " + "\n  - ".join(missing))


def cmd_sync_gateway(args):
    gateway_root = Path(os.environ.get("GATEWAY_ROOT", DEFAULT_GATEWAY_ROOT))
    router_path = gateway_root / "layer-gateway/litellm-orch/config/router.yaml"
    env_path = gateway_root / "layer-gateway/litellm-orch/config/env.local"
    handles_path = gateway_root / "layer-gateway/registry/handles.jsonl"

    if not router_path.exists():
        gateway_host = os.environ.get("GATEWAY_HOST", DEFAULT_GATEWAY_HOST)
        gateway_mlxctl = os.environ.get("GATEWAY_MLXCTL", DEFAULT_GATEWAY_MLXCTL)
        remote_cmd = [gateway_mlxctl, "--local", "sync-gateway"]
        if args.dry_run:
            remote_cmd.append("--dry-run")
        if args.route_via_optillm is True:
            remote_cmd.append("--route-via-optillm")
        elif args.route_via_optillm is False:
            remote_cmd.append("--no-route-via-optillm")
        subprocess.run(["ssh", gateway_host] + remote_cmd, check=False)
        return

    studio_ssh = os.environ.get("STUDIO_SSH", "thestudio@192.168.1.72")
    registry_path = os.environ.get("MLX_REGISTRY_PATH", DEFAULT_REGISTRY)
    registry = _fetch_registry_from_studio(studio_ssh, registry_path)
    mlx_entries = _build_mlx_entries(registry.get("models", {}), os.environ.get("MLX_HOST", DEFAULT_MLX_HOST))
    route_via_optillm = args.route_via_optillm
    if route_via_optillm is None:
        env_flag = os.environ.get("MLX_ROUTE_VIA_OPTILLM")
        if env_flag is None:
            route_via_optillm = DEFAULT_ROUTE_VIA_OPTILLM
        else:
            route_via_optillm = env_flag.strip().lower() not in {"0", "false", "no"}

    if args.dry_run:
        print(json.dumps({"count": len(mlx_entries), "entries": mlx_entries}, indent=2))
        return

    _update_router_yaml(router_path, mlx_entries, route_via_optillm)
    _update_env_local(env_path, mlx_entries, os.environ.get("MLX_HOST", DEFAULT_MLX_HOST), route_via_optillm)
    _update_handles(handles_path, mlx_entries)


def _collect_offload_candidates(studio_ssh: str):
    script = """
import json
from pathlib import Path

hub = Path("/Users/thestudio/models/hf/hub")
reg = hub / "registry.json"
if not reg.exists():
    raise SystemExit(0)
data = json.loads(reg.read_text() or "{}")
models = data.get("models", {})
for entry in models.values():
    og_path = entry.get("og_path")
    cache_path = entry.get("cache_path")
    if not og_path or not cache_path:
        continue
    if og_path == cache_path:
        continue
    try:
        og_path = Path(og_path)
        rel = og_path.relative_to(hub)
    except Exception:
        continue
    print(str(rel))
"""
    result = subprocess.run(
        ["ssh", studio_ssh, "python3", "-"],
        input=script.encode("utf-8"),
        capture_output=True,
        check=False,
    )
    if result.returncode != 0:
        raise SystemExit("failed to build offload list from studio registry")
    lines = [line.strip() for line in result.stdout.decode("utf-8").splitlines() if line.strip()]
    return lines


def cmd_offload_og(args):
    studio_ssh = os.environ.get("STUDIO_SSH", "thestudio@192.168.1.72")
    offload_script = Path(os.environ.get("MLX_OFFLOAD_SCRIPT", DEFAULT_OFFLOAD_SCRIPT))

    candidates = _collect_offload_candidates(studio_ssh)
    if not candidates:
        print("no offload candidates found")
        return
    if args.dry_run:
        print("\n".join(candidates))
        return

    with tempfile.NamedTemporaryFile("w", delete=False) as handle:
        for line in candidates:
            handle.write(line + "\n")
        list_path = handle.name

    if offload_script.exists():
        env = os.environ.copy()
        env["MLX_OFFLOAD_ALLOW"] = "1"
        env["MLX_OFFLOAD_LIST"] = list_path
        env.setdefault("STUDIO_SSH", studio_ssh)
        subprocess.run([str(offload_script)], env=env, check=False)
        return

    offload_host = os.environ.get("MLX_OFFLOAD_HOST", "mini")
    if not offload_host:
        raise SystemExit(f"offload script not found locally and MLX_OFFLOAD_HOST not set: {offload_script}")

    remote_list = f"/tmp/mlx_offload_{os.getpid()}.txt"
    subprocess.run(
        ["ssh", offload_host, f"cat > {remote_list}"],
        input=Path(list_path).read_bytes(),
        check=False,
    )
    env_parts = [
        "MLX_OFFLOAD_ALLOW=1",
        f"MLX_OFFLOAD_LIST={remote_list}",
        f"STUDIO_SSH={studio_ssh}",
    ]
    subprocess.run(
        ["ssh", offload_host, " ".join(env_parts + [str(offload_script)])],
        check=False,
    )
    subprocess.run(["ssh", offload_host, "rm", "-f", remote_list], check=False)


def main():
    parser = argparse.ArgumentParser(prog="mlxctl")
    parser.add_argument("--local", action="store_true", help="run locally on Studio")
    sub = parser.add_subparsers(dest="command", required=True)

    sub.add_parser("init")
    sub.add_parser("list", help="List registered MLX models (model_id, repo_id, port)")
    sub.add_parser("status", help="Show MLX port status and OptiLLM routing mode")

    load = sub.add_parser("load", help="Download/register a model and launch it on a port")
    load.add_argument("model")
    load.add_argument("port", help="Port number (8100-8139) or 'auto' for 8120+")
    load.add_argument("--force", action="store_true", help="Force reload even if port/model is already assigned")
    load.add_argument("--ignore-launchd", action="store_true", help="Skip launchd stop prompt")
    load.add_argument("--convert", choices=["auto", "force", "skip"], default="auto")
    load.add_argument("--name", help="Canonical model id override")
    load.add_argument("--no-offload-og", dest="offload_og", action="store_false", help="Skip offloading OG weights after conversion")
    load.set_defaults(offload_og=True)
    load.add_argument(
        "--preconverted",
        choices=["yes", "no", "auto"],
        default="auto",
        help="Override pre-converted detection (yes|no|auto)",
    )
    load.add_argument("--no-sync", dest="sync", action="store_false", help="Skip gateway sync after load")
    load.set_defaults(sync=True)

    unload = sub.add_parser("unload")
    unload.add_argument("port")
    unload.add_argument("--no-sync", dest="sync", action="store_false", help="Skip gateway sync after unload")
    unload.set_defaults(sync=True)

    unload_all = sub.add_parser("unload-all")
    unload_all.add_argument("--no-sync", dest="sync", action="store_false", help="Skip gateway sync after unload-all")
    unload_all.set_defaults(sync=True)
    sub.add_parser("reconcile")
    sub.add_parser("verify")
    # Deprecated: ensemble (kept for backward compatibility)
    ensemble = sub.add_parser("ensemble", help="DEPRECATED: use load/assign-team instead")
    ensemble.add_argument("name", help="Deprecated")
    ensemble.add_argument("--force", action="store_true")
    ensemble.add_argument("--ignore-launchd", action="store_true")
    ensure = sub.add_parser("ensure", help="Ensure model is present in registry (optionally load)")
    ensure.add_argument("repo")
    ensure.add_argument("--name", help="Canonical model id override")
    ensure.add_argument("--convert", choices=["auto", "force", "skip"], default="auto")
    ensure.add_argument("--no-offload-og", dest="offload_og", action="store_false")
    ensure.set_defaults(offload_og=True)
    ensure.add_argument(
        "--preconverted",
        choices=["yes", "no", "auto"],
        default="auto",
        help="Override pre-converted detection (yes|no|auto)",
    )
    ensure.add_argument("--load", action="store_true", help="Load the model after ensuring it")
    ensure.add_argument("--port", default="auto", help="Port number (8100-8139) or 'auto' for 8120+")
    ensure.add_argument("--no-sync", dest="sync", action="store_false")
    ensure.set_defaults(sync=True)
    offload = sub.add_parser("offload-og", help="Offload OG base weights that are not used for inference")
    offload.add_argument("--dry-run", action="store_true")
    sync = sub.add_parser("sync-gateway", help="Sync MLX registry to gateway router/env/handles")
    sync.add_argument("--route-via-optillm", dest="route_via_optillm", action="store_true", help="Route MLX handles through OptiLLM (default)")
    sync.add_argument("--no-route-via-optillm", dest="route_via_optillm", action="store_false", help="Route MLX handles directly to MLX ports")
    sync.set_defaults(route_via_optillm=None)
    sync.add_argument("--dry-run", action="store_true")
    assign = sub.add_parser("assign-team", help="Assign MLX models to team ports by size (OptiLLM first)")
    assign.add_argument("--dry-run", action="store_true")
    assign.add_argument("--no-sync", dest="sync", action="store_false")
    assign.set_defaults(sync=True)

    args = parser.parse_args()
    _maybe_forward_to_studio(args)

    commands = {
        "init": cmd_init,
        "list": cmd_list,
        "status": cmd_status,
        "load": cmd_load,
        "unload": cmd_unload,
        "unload-all": cmd_unload_all,
        "reconcile": cmd_reconcile,
        "verify": cmd_verify,
        "ensemble": cmd_ensemble,
        "ensure": lambda args: cmd_ensure(args),
        "offload-og": cmd_offload_og,
        "sync-gateway": cmd_sync_gateway,
        "assign-team": cmd_assign_team,
    }
    commands[args.command](args)


if __name__ == "__main__":
    main()
